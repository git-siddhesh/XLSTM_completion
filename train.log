2024-08-11 17:17:40,909 [INFO] --------------------------------------------------------------------------------
2024-08-11 17:17:40,909 [INFO] Starting training
2024-08-11 17:17:41,840 [INFO] Subsetting the dataset to 1000
2024-08-11 17:17:41,840 [INFO] Dataset size: 1000
2024-08-11 17:17:41,841 [INFO] Train dataset size: 800
2024-08-11 17:17:41,841 [INFO] Test dataset size: 200
2024-08-11 17:17:41,841 [INFO] Batch size: 32
2024-08-11 17:17:41,841 [INFO] Batches in train: 25.0
2024-08-11 17:17:41,841 [INFO] Batches in test: 6.25
2024-08-11 17:17:48,076 [INFO] Before compilation and loading of slstm.
2024-08-11 17:17:48,119 [INFO] After compilation and loading of slstm.
2024-08-11 17:17:55,950 [INFO] Before compilation and loading of slstm.
2024-08-11 17:17:55,969 [INFO] After compilation and loading of slstm.
2024-08-11 17:17:56,182 [INFO] Model loaded
2024-08-11 17:17:56,184 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-11 17:17:56,185 [INFO] Epochs 10
2024-08-11 17:17:56,185 [INFO] Starting training
2024-08-11 17:17:56,185 [INFO] Epoch 0 started at Sun Aug 11 17:17:56 2024
2024-08-11 17:18:14,976 [INFO] --------------------------------------------------------------------------------
2024-08-11 17:18:14,976 [INFO] Starting training
2024-08-11 17:18:15,904 [INFO] Subsetting the dataset to 1000
2024-08-11 17:18:15,904 [INFO] Dataset size: 1000
2024-08-11 17:18:15,905 [INFO] Train dataset size: 800
2024-08-11 17:18:15,905 [INFO] Test dataset size: 200
2024-08-11 17:18:15,905 [INFO] Batch size: 16
2024-08-11 17:18:15,905 [INFO] Batches in train: 50.0
2024-08-11 17:18:15,905 [INFO] Batches in test: 12.5
2024-08-11 17:18:20,687 [INFO] Before compilation and loading of slstm.
2024-08-11 17:18:20,726 [INFO] After compilation and loading of slstm.
2024-08-11 17:18:21,602 [INFO] Before compilation and loading of slstm.
2024-08-11 17:18:21,621 [INFO] After compilation and loading of slstm.
2024-08-11 17:18:21,819 [INFO] Model loaded
2024-08-11 17:18:21,820 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-11 17:18:21,821 [INFO] Epochs 10
2024-08-11 17:18:21,821 [INFO] Starting training
2024-08-11 17:18:21,821 [INFO] Epoch 0 started at Sun Aug 11 17:18:21 2024
2024-08-11 17:18:36,584 [INFO] --------------------------------------------------------------------------------
2024-08-11 17:18:36,585 [INFO] Starting training
2024-08-11 17:18:37,517 [INFO] Subsetting the dataset to 1000
2024-08-11 17:18:37,517 [INFO] Dataset size: 1000
2024-08-11 17:18:37,517 [INFO] Train dataset size: 800
2024-08-11 17:18:37,517 [INFO] Test dataset size: 200
2024-08-11 17:18:37,518 [INFO] Batch size: 8
2024-08-11 17:18:37,518 [INFO] Batches in train: 100.0
2024-08-11 17:18:37,518 [INFO] Batches in test: 25.0
2024-08-11 17:18:46,548 [INFO] Before compilation and loading of slstm.
2024-08-11 17:18:46,588 [INFO] After compilation and loading of slstm.
2024-08-11 17:18:48,218 [INFO] Before compilation and loading of slstm.
2024-08-11 17:18:48,236 [INFO] After compilation and loading of slstm.
2024-08-11 17:18:48,408 [INFO] Model loaded
2024-08-11 17:18:48,409 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-11 17:18:48,410 [INFO] Epochs 10
2024-08-11 17:18:48,410 [INFO] Starting training
2024-08-11 17:18:48,410 [INFO] Epoch 0 started at Sun Aug 11 17:18:48 2024
2024-08-11 17:18:50,665 [INFO] Epoch and iter 0 0 Loss 107.08622741699219 Time 2.2551138401031494
2024-08-11 17:19:02,782 [INFO] --------------------------------------------------------------------------------
2024-08-11 17:19:02,783 [INFO] Starting training
2024-08-11 17:19:03,681 [INFO] Subsetting the dataset to 1000
2024-08-11 17:19:03,681 [INFO] Dataset size: 1000
2024-08-11 17:19:03,682 [INFO] Train dataset size: 800
2024-08-11 17:19:03,682 [INFO] Test dataset size: 200
2024-08-11 17:19:03,682 [INFO] Batch size: 12
2024-08-11 17:19:03,682 [INFO] Batches in train: 66.66666666666667
2024-08-11 17:19:03,682 [INFO] Batches in test: 16.666666666666668
2024-08-11 17:19:08,441 [INFO] Before compilation and loading of slstm.
2024-08-11 17:19:08,744 [INFO] After compilation and loading of slstm.
2024-08-11 17:19:13,861 [INFO] Before compilation and loading of slstm.
2024-08-11 17:19:13,880 [INFO] After compilation and loading of slstm.
2024-08-11 17:19:14,059 [INFO] Model loaded
2024-08-11 17:19:14,061 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-11 17:19:14,061 [INFO] Epochs 10
2024-08-11 17:19:14,061 [INFO] Starting training
2024-08-11 17:19:14,061 [INFO] Epoch 0 started at Sun Aug 11 17:19:14 2024
2024-08-11 17:19:16,577 [INFO] Epoch and iter 0 0 Loss 176.750244140625 Time 2.515671730041504
2024-08-11 17:19:25,363 [INFO] Epoch and iter 0 10 Loss 39.08363342285156 Time 8.78210997581482
2024-08-11 17:19:34,155 [INFO] Epoch and iter 0 20 Loss 25.743242263793945 Time 8.789019584655762
2024-08-11 17:19:43,080 [INFO] --------------------------------------------------------------------------------
2024-08-11 17:19:43,080 [INFO] Starting training
2024-08-11 17:19:44,007 [INFO] Subsetting the dataset to 1000
2024-08-11 17:19:44,007 [INFO] Dataset size: 1000
2024-08-11 17:19:44,008 [INFO] Train dataset size: 800
2024-08-11 17:19:44,008 [INFO] Test dataset size: 200
2024-08-11 17:19:44,008 [INFO] Batch size: 14
2024-08-11 17:19:44,008 [INFO] Batches in train: 57.142857142857146
2024-08-11 17:19:44,008 [INFO] Batches in test: 14.285714285714286
2024-08-11 17:19:50,280 [INFO] Before compilation and loading of slstm.
2024-08-11 17:19:50,320 [INFO] After compilation and loading of slstm.
2024-08-11 17:19:55,028 [INFO] Before compilation and loading of slstm.
2024-08-11 17:19:55,045 [INFO] After compilation and loading of slstm.
2024-08-11 17:19:55,212 [INFO] Model loaded
2024-08-11 17:19:55,213 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-11 17:19:55,214 [INFO] Epochs 10
2024-08-11 17:19:55,214 [INFO] Starting training
2024-08-11 17:19:55,214 [INFO] Epoch 0 started at Sun Aug 11 17:19:55 2024
2024-08-11 17:19:57,949 [INFO] Epoch and iter 0 0 Loss 190.01295471191406 Time 2.735252618789673
2024-08-11 17:20:08,091 [INFO] Epoch and iter 0 10 Loss 40.62837219238281 Time 10.137939929962158
2024-08-11 17:20:18,232 [INFO] Epoch and iter 0 20 Loss 32.346805572509766 Time 10.13722014427185
2024-08-11 17:20:28,393 [INFO] Epoch and iter 0 30 Loss 28.262937545776367 Time 10.157336235046387
2024-08-11 17:20:38,561 [INFO] Epoch and iter 0 40 Loss 33.40074920654297 Time 10.164576768875122
2024-08-11 17:20:48,731 [INFO] Epoch and iter 0 50 Loss 22.196369171142578 Time 10.16649341583252
2024-08-11 17:20:55,095 [INFO] Saving this epoch
2024-08-11 17:20:55,169 [INFO] Epoch 1 started at Sun Aug 11 17:20:55 2024
2024-08-11 17:20:56,259 [INFO] Epoch and iter 1 0 Loss 34.96735763549805 Time 7.525207757949829
2024-08-11 17:21:06,443 [INFO] Epoch and iter 1 10 Loss 33.496116638183594 Time 10.179889678955078
2024-08-11 17:21:16,623 [INFO] Epoch and iter 1 20 Loss 35.74556350708008 Time 10.176501274108887
2024-08-11 17:21:26,804 [INFO] Epoch and iter 1 30 Loss 41.966148376464844 Time 10.17813754081726
2024-08-11 17:21:36,986 [INFO] Epoch and iter 1 40 Loss 31.835582733154297 Time 10.178554058074951
2024-08-11 17:21:47,171 [INFO] Epoch and iter 1 50 Loss 25.449687957763672 Time 10.181127548217773
2024-08-11 17:21:53,588 [INFO] Saving this epoch
2024-08-11 17:21:53,657 [INFO] Epoch 2 started at Sun Aug 11 17:21:53 2024
2024-08-11 17:21:54,752 [INFO] Epoch and iter 2 0 Loss 26.763526916503906 Time 7.577729225158691
2024-08-11 17:22:04,937 [INFO] Epoch and iter 2 10 Loss 24.707473754882812 Time 10.181246042251587
2024-08-11 17:22:15,120 [INFO] Epoch and iter 2 20 Loss 30.882057189941406 Time 10.179595232009888
2024-08-11 17:22:25,308 [INFO] Epoch and iter 2 30 Loss 31.3154296875 Time 10.184857845306396
2024-08-11 17:22:35,495 [INFO] Epoch and iter 2 40 Loss 36.218994140625 Time 10.18306016921997
2024-08-11 17:22:45,683 [INFO] Epoch and iter 2 50 Loss 28.260438919067383 Time 10.184854507446289
2024-08-11 17:22:52,105 [INFO] Saving this epoch
2024-08-11 17:22:52,175 [INFO] Epoch 3 started at Sun Aug 11 17:22:52 2024
2024-08-11 17:22:53,268 [INFO] Epoch and iter 3 0 Loss 25.84512710571289 Time 7.581915855407715
2024-08-11 17:23:03,460 [INFO] Epoch and iter 3 10 Loss 24.987810134887695 Time 10.188135385513306
2024-08-11 17:23:13,653 [INFO] Epoch and iter 3 20 Loss 23.216371536254883 Time 10.189600229263306
2024-08-11 17:23:23,839 [INFO] Epoch and iter 3 30 Loss 18.541908264160156 Time 10.183339357376099
2024-08-11 17:23:34,028 [INFO] Epoch and iter 3 40 Loss 15.941315650939941 Time 10.185075521469116
2024-08-11 17:23:44,221 [INFO] Epoch and iter 3 50 Loss 23.859272003173828 Time 10.189508199691772
2024-08-11 17:23:50,641 [INFO] Saving this epoch
2024-08-11 17:23:50,709 [INFO] Epoch 4 started at Sun Aug 11 17:23:50 2024
2024-08-11 17:23:51,804 [INFO] Epoch and iter 4 0 Loss 33.466339111328125 Time 7.5798516273498535
2024-08-11 17:24:01,999 [INFO] Epoch and iter 4 10 Loss 24.072181701660156 Time 10.190913915634155
2024-08-11 17:24:12,194 [INFO] Epoch and iter 4 20 Loss 19.45046615600586 Time 10.191826581954956
2024-08-11 17:24:22,387 [INFO] Epoch and iter 4 30 Loss 19.299877166748047 Time 10.18994951248169
2024-08-11 17:24:32,581 [INFO] Epoch and iter 4 40 Loss 20.2965087890625 Time 10.190995931625366
2024-08-11 17:24:42,774 [INFO] Epoch and iter 4 50 Loss 19.58133888244629 Time 10.189115524291992
2024-08-11 17:24:49,190 [INFO] Saving this epoch
2024-08-11 17:24:49,258 [INFO] Epoch 5 started at Sun Aug 11 17:24:49 2024
2024-08-11 17:24:50,344 [INFO] Epoch and iter 5 0 Loss 26.049236297607422 Time 7.56695818901062
2024-08-11 17:25:00,539 [INFO] Epoch and iter 5 10 Loss 19.955482482910156 Time 10.190807104110718
2024-08-11 17:25:10,732 [INFO] Epoch and iter 5 20 Loss 28.66413116455078 Time 10.189579725265503
2024-08-11 17:25:20,925 [INFO] Epoch and iter 5 30 Loss 20.882389068603516 Time 10.18983006477356
2024-08-11 17:25:31,115 [INFO] Epoch and iter 5 40 Loss 19.53178596496582 Time 10.186684370040894
2024-08-11 17:25:41,309 [INFO] Epoch and iter 5 50 Loss 15.831073760986328 Time 10.190807819366455
2024-08-11 17:25:47,708 [INFO] Saving this epoch
2024-08-11 17:25:47,775 [INFO] Epoch 6 started at Sun Aug 11 17:25:47 2024
2024-08-11 17:25:48,868 [INFO] Epoch and iter 6 0 Loss 22.31000328063965 Time 7.555668592453003
2024-08-11 17:25:59,064 [INFO] Epoch and iter 6 10 Loss 16.605480194091797 Time 10.192104816436768
2024-08-11 17:26:09,257 [INFO] Epoch and iter 6 20 Loss 28.410364151000977 Time 10.18979787826538
2024-08-11 17:26:19,592 [INFO] Epoch and iter 6 30 Loss 27.948135375976562 Time 10.331785917282104
2024-08-11 17:26:29,779 [INFO] Epoch and iter 6 40 Loss 20.75796127319336 Time 10.182981729507446
2024-08-11 17:26:39,964 [INFO] Epoch and iter 6 50 Loss 17.678253173828125 Time 10.182053327560425
2024-08-11 17:26:46,381 [INFO] Saving this epoch
2024-08-11 17:26:46,448 [INFO] Epoch 7 started at Sun Aug 11 17:26:46 2024
2024-08-11 17:26:47,542 [INFO] Epoch and iter 7 0 Loss 22.21561050415039 Time 7.574168682098389
2024-08-11 17:26:57,888 [INFO] Epoch and iter 7 10 Loss 19.03024673461914 Time 10.342535972595215
2024-08-11 17:27:08,233 [INFO] Epoch and iter 7 20 Loss 20.144817352294922 Time 10.34152340888977
2024-08-11 17:27:18,424 [INFO] Epoch and iter 7 30 Loss 16.195018768310547 Time 10.188188552856445
2024-08-11 17:27:28,614 [INFO] Epoch and iter 7 40 Loss 16.559982299804688 Time 10.18633508682251
2024-08-11 17:27:38,800 [INFO] Epoch and iter 7 50 Loss 14.97784423828125 Time 10.182771444320679
2024-08-11 17:27:45,212 [INFO] Saving this epoch
2024-08-11 17:27:45,279 [INFO] Epoch 8 started at Sun Aug 11 17:27:45 2024
2024-08-11 17:27:46,372 [INFO] Epoch and iter 8 0 Loss 16.574180603027344 Time 7.568246364593506
2024-08-11 17:27:56,566 [INFO] Epoch and iter 8 10 Loss 18.87982749938965 Time 10.19067668914795
2024-08-11 17:28:06,752 [INFO] Epoch and iter 8 20 Loss 19.248558044433594 Time 10.182249546051025
2024-08-11 17:28:16,941 [INFO] Epoch and iter 8 30 Loss 18.946426391601562 Time 10.18553113937378
2024-08-11 17:28:27,126 [INFO] Epoch and iter 8 40 Loss 21.61690330505371 Time 10.181612491607666
2024-08-11 17:28:37,311 [INFO] Epoch and iter 8 50 Loss 20.642749786376953 Time 10.181894540786743
2024-08-11 17:28:43,723 [INFO] Saving this epoch
2024-08-11 17:28:43,790 [INFO] Epoch 9 started at Sun Aug 11 17:28:43 2024
2024-08-11 17:28:44,882 [INFO] Epoch and iter 9 0 Loss 18.566932678222656 Time 7.567572116851807
2024-08-11 17:28:55,068 [INFO] Epoch and iter 9 10 Loss 22.796823501586914 Time 10.182853937149048
2024-08-11 17:29:05,252 [INFO] Epoch and iter 9 20 Loss 17.042011260986328 Time 10.180150985717773
2024-08-11 17:29:15,437 [INFO] Epoch and iter 9 30 Loss 16.71957015991211 Time 10.181500434875488
2024-08-11 17:29:25,615 [INFO] Epoch and iter 9 40 Loss 33.558349609375 Time 10.175416469573975
2024-08-11 17:29:35,796 [INFO] Epoch and iter 9 50 Loss 22.59004020690918 Time 10.176967859268188
2024-08-11 17:29:42,211 [INFO] Saving this epoch
2024-08-11 17:29:42,278 [INFO] Saving model
2024-08-11 17:29:42,342 [INFO] Testing
2024-08-11 17:29:46,102 [INFO] Average chamfer distance on val 0.0004897490201983601
2024-08-13 13:09:37,094 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:09:37,094 [INFO] Starting training
2024-08-13 13:09:40,629 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7fe7036f40d0>>, run_name : dummy, 123456
2024-08-13 13:09:41,510 [INFO] Dataset size: 62400
2024-08-13 13:09:41,515 [INFO] Train dataset size: 49920
2024-08-13 13:09:41,515 [INFO] Test dataset size: 12480
2024-08-13 13:09:41,515 [INFO] Batch size: 14
2024-08-13 13:09:41,515 [INFO] Batches in train: 3565.714285714286
2024-08-13 13:09:41,515 [INFO] Batches in test: 891.4285714285714
2024-08-13 13:09:51,476 [INFO] Before compilation and loading of slstm.
2024-08-13 13:09:51,517 [INFO] After compilation and loading of slstm.
2024-08-13 13:09:54,447 [INFO] Before compilation and loading of slstm.
2024-08-13 13:09:54,466 [INFO] After compilation and loading of slstm.
2024-08-13 13:09:54,640 [INFO] Model loaded
2024-08-13 13:09:54,642 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:09:54,643 [INFO] Epochs 50
2024-08-13 13:09:54,643 [INFO] Starting training
2024-08-13 13:09:54,643 [INFO] Epoch 0 started at Tue Aug 13 13:09:54 2024
2024-08-13 13:10:33,154 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:10:33,155 [INFO] Starting training
2024-08-13 13:10:36,564 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f4bef3e0100>>, run_name : dummy, 123456
2024-08-13 13:10:37,446 [INFO] Dataset size: 62400
2024-08-13 13:10:37,450 [INFO] Train dataset size: 49920
2024-08-13 13:10:37,450 [INFO] Test dataset size: 12480
2024-08-13 13:10:37,451 [INFO] Batch size: 14
2024-08-13 13:10:37,451 [INFO] Batches in train: 3565.714285714286
2024-08-13 13:10:37,451 [INFO] Batches in test: 891.4285714285714
2024-08-13 13:10:43,979 [INFO] Before compilation and loading of slstm.
2024-08-13 13:10:44,024 [INFO] After compilation and loading of slstm.
2024-08-13 13:10:52,767 [INFO] Before compilation and loading of slstm.
2024-08-13 13:10:52,789 [INFO] After compilation and loading of slstm.
2024-08-13 13:10:52,987 [INFO] Model loaded
2024-08-13 13:10:52,989 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:10:52,990 [INFO] Epochs 50
2024-08-13 13:10:52,990 [INFO] Starting training
2024-08-13 13:10:52,990 [INFO] Epoch 0 started at Tue Aug 13 13:10:52 2024
2024-08-13 13:12:01,194 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:12:01,194 [INFO] Starting training
2024-08-13 13:12:04,657 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f373d3a4100>>, run_name : dummy, 123456
2024-08-13 13:12:05,538 [INFO] Dataset size: 62400
2024-08-13 13:12:05,554 [INFO] Train dataset size: 49920
2024-08-13 13:12:05,554 [INFO] Test dataset size: 12480
2024-08-13 13:12:05,555 [INFO] Batch size: 14
2024-08-13 13:12:05,555 [INFO] Batches in train: 3565.714285714286
2024-08-13 13:12:05,555 [INFO] Batches in test: 891.4285714285714
2024-08-13 13:12:06,948 [INFO] Before compilation and loading of slstm.
2024-08-13 13:12:06,990 [INFO] After compilation and loading of slstm.
2024-08-13 13:12:12,423 [INFO] Before compilation and loading of slstm.
2024-08-13 13:12:12,443 [INFO] After compilation and loading of slstm.
2024-08-13 13:12:12,608 [INFO] Model loaded
2024-08-13 13:12:12,609 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:12:12,610 [INFO] Epochs 50
2024-08-13 13:12:12,610 [INFO] Starting training
2024-08-13 13:12:12,610 [INFO] Epoch 0 started at Tue Aug 13 13:12:12 2024
2024-08-13 13:12:15,104 [INFO] Epoch and iter 0 0 Loss 389.7576599121094 Time 2.493847370147705
2024-08-13 13:19:52,273 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:19:52,274 [INFO] Starting training
2024-08-13 13:19:56,045 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f9fc7108520>>, run_name : dummy, 123456
2024-08-13 13:19:56,925 [INFO] Subsetting the dataset to 1000
2024-08-13 13:19:56,925 [INFO] Dataset size: 1000
2024-08-13 13:19:56,925 [INFO] Train dataset size: 800
2024-08-13 13:19:56,926 [INFO] Test dataset size: 200
2024-08-13 13:19:56,926 [INFO] Batch size: 14
2024-08-13 13:19:56,926 [INFO] Batches in train: 57.142857142857146
2024-08-13 13:19:56,926 [INFO] Batches in test: 14.285714285714286
2024-08-13 13:20:06,671 [INFO] Before compilation and loading of slstm.
2024-08-13 13:20:06,717 [INFO] After compilation and loading of slstm.
2024-08-13 13:20:10,531 [INFO] Before compilation and loading of slstm.
2024-08-13 13:20:10,550 [INFO] After compilation and loading of slstm.
2024-08-13 13:20:10,719 [INFO] Model loaded
2024-08-13 13:20:10,720 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:20:10,721 [INFO] Epochs 50
2024-08-13 13:20:10,721 [INFO] Starting training
2024-08-13 13:20:10,721 [INFO] Epoch 0 started at Tue Aug 13 13:20:10 2024
2024-08-13 13:21:10,039 [INFO] Saving this epoch  0 +1
2024-08-13 13:21:10,443 [INFO] Epoch 1 started at Tue Aug 13 13:21:10 2024
2024-08-13 13:21:53,306 [INFO] Epoch and iter 1 41 Loss 31.4415283203125 Time 102.58473300933838
2024-08-13 13:22:08,883 [INFO] Saving this epoch  1 +1
2024-08-13 13:22:09,284 [INFO] Plotting the results at epoch 1
2024-08-13 13:31:11,266 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:31:11,266 [INFO] Starting training
2024-08-13 13:31:14,866 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f689faf45e0>>, run_name : dummy, 123456
2024-08-13 13:31:15,750 [INFO] Subsetting the dataset to 1000
2024-08-13 13:31:15,750 [INFO] Dataset size: 1000
2024-08-13 13:31:15,751 [INFO] Train dataset size: 800
2024-08-13 13:31:15,751 [INFO] Test dataset size: 200
2024-08-13 13:31:15,751 [INFO] Batch size: 14
2024-08-13 13:31:15,751 [INFO] Batches in train: 57.142857142857146
2024-08-13 13:31:15,751 [INFO] Batches in test: 14.285714285714286
2024-08-13 13:31:19,418 [INFO] Before compilation and loading of slstm.
2024-08-13 13:31:19,460 [INFO] After compilation and loading of slstm.
2024-08-13 13:31:27,752 [INFO] Before compilation and loading of slstm.
2024-08-13 13:31:27,772 [INFO] After compilation and loading of slstm.
2024-08-13 13:31:27,973 [INFO] Model loaded
2024-08-13 13:31:27,975 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:31:27,976 [INFO] Epochs 50
2024-08-13 13:31:27,976 [INFO] Starting training
2024-08-13 13:31:27,976 [INFO] Epoch 0 started at Tue Aug 13 13:31:27 2024
2024-08-13 13:32:27,393 [INFO] Saving this epoch  0 +1
2024-08-13 13:32:27,801 [INFO] Epoch 1 started at Tue Aug 13 13:32:27 2024
2024-08-13 13:33:10,670 [INFO] Epoch and iter 1 41 Loss 20.643171310424805 Time 102.6942732334137
2024-08-13 13:35:41,564 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:35:41,565 [INFO] Starting training
2024-08-13 13:36:00,264 [INFO] --------------------------------------------------------------------------------
2024-08-13 13:36:00,264 [INFO] Starting training
2024-08-13 13:36:04,061 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f21cdacc460>>, run_name : main, 123456
2024-08-13 13:36:04,951 [INFO] Dataset size: 62400
2024-08-13 13:36:04,956 [INFO] Train dataset size: 49920
2024-08-13 13:36:04,956 [INFO] Test dataset size: 12480
2024-08-13 13:36:04,956 [INFO] Batch size: 14
2024-08-13 13:36:04,956 [INFO] Batches in train: 3565.714285714286
2024-08-13 13:36:04,956 [INFO] Batches in test: 891.4285714285714
2024-08-13 13:36:08,792 [INFO] Before compilation and loading of slstm.
2024-08-13 13:36:08,834 [INFO] After compilation and loading of slstm.
2024-08-13 13:36:15,633 [INFO] Before compilation and loading of slstm.
2024-08-13 13:36:15,653 [INFO] After compilation and loading of slstm.
2024-08-13 13:36:15,818 [INFO] Model loaded
2024-08-13 13:36:15,820 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-13 13:36:15,821 [INFO] Epochs 50
2024-08-13 13:36:15,821 [INFO] Starting training
2024-08-13 13:36:15,821 [INFO] Epoch 0 started at Tue Aug 13 13:36:15 2024
2024-08-13 13:37:58,866 [INFO] Epoch and iter 0 99 Loss 119.7412338256836 Time 103.04495692253113
2024-08-13 13:39:40,861 [INFO] Epoch and iter 0 199 Loss 98.3315658569336 Time 101.99387454986572
2024-08-13 13:41:22,894 [INFO] Epoch and iter 0 299 Loss 108.37017059326172 Time 102.03254508972168
2024-08-13 13:43:04,906 [INFO] Epoch and iter 0 399 Loss 74.64309692382812 Time 102.01124691963196
2024-08-13 13:44:46,844 [INFO] Epoch and iter 0 499 Loss 71.54011535644531 Time 101.93704438209534
2024-08-13 13:46:28,755 [INFO] Epoch and iter 0 599 Loss 50.344520568847656 Time 101.91024827957153
2024-08-13 13:48:10,660 [INFO] Epoch and iter 0 699 Loss 64.52649688720703 Time 101.90418910980225
2024-08-13 13:49:52,587 [INFO] Epoch and iter 0 799 Loss 38.52144241333008 Time 101.92644000053406
2024-08-13 13:51:34,497 [INFO] Epoch and iter 0 899 Loss 46.44972229003906 Time 101.90914797782898
2024-08-13 13:53:16,419 [INFO] Epoch and iter 0 999 Loss 71.0480728149414 Time 101.9214460849762
2024-08-13 13:54:58,342 [INFO] Epoch and iter 0 1099 Loss 63.28877258300781 Time 101.92188096046448
2024-08-13 13:56:40,273 [INFO] Epoch and iter 0 1199 Loss 50.61936950683594 Time 101.92971444129944
2024-08-13 13:58:22,201 [INFO] Epoch and iter 0 1299 Loss 39.735660552978516 Time 101.92697811126709
2024-08-13 14:00:04,125 [INFO] Epoch and iter 0 1399 Loss 44.59988784790039 Time 101.9238977432251
2024-08-13 14:01:46,038 [INFO] Epoch and iter 0 1499 Loss 47.687950134277344 Time 101.91234040260315
2024-08-13 14:03:27,978 [INFO] Epoch and iter 0 1599 Loss 54.90208435058594 Time 101.9392921924591
2024-08-13 14:05:09,932 [INFO] Epoch and iter 0 1699 Loss 48.48255157470703 Time 101.95302629470825
2024-08-13 14:06:51,867 [INFO] Epoch and iter 0 1799 Loss 48.80992126464844 Time 101.93358087539673
2024-08-13 14:08:33,792 [INFO] Epoch and iter 0 1899 Loss 40.220375061035156 Time 101.9240882396698
2024-08-13 14:10:15,716 [INFO] Epoch and iter 0 1999 Loss 47.181129455566406 Time 101.92400169372559
2024-08-13 14:11:57,654 [INFO] Epoch and iter 0 2099 Loss 41.43324279785156 Time 101.9367311000824
2024-08-13 14:13:39,582 [INFO] Epoch and iter 0 2199 Loss 39.714717864990234 Time 101.92758464813232
2024-08-13 14:15:21,525 [INFO] Epoch and iter 0 2299 Loss 45.616329193115234 Time 101.94220185279846
2024-08-13 14:17:03,462 [INFO] Epoch and iter 0 2399 Loss 64.44062042236328 Time 101.93548464775085
2024-08-13 14:18:45,398 [INFO] Epoch and iter 0 2499 Loss 45.94596481323242 Time 101.93522620201111
2024-08-13 14:20:27,322 [INFO] Epoch and iter 0 2599 Loss 34.70933151245117 Time 101.92339181900024
2024-08-13 14:22:09,251 [INFO] Epoch and iter 0 2699 Loss 44.11924743652344 Time 101.92871308326721
2024-08-13 14:23:51,168 [INFO] Epoch and iter 0 2799 Loss 38.91284942626953 Time 101.91545343399048
2024-08-13 14:25:33,096 [INFO] Epoch and iter 0 2899 Loss 44.71366882324219 Time 101.92799139022827
2024-08-13 14:27:15,026 [INFO] Epoch and iter 0 2999 Loss 27.544170379638672 Time 101.92891645431519
2024-08-13 14:28:56,958 [INFO] Epoch and iter 0 3099 Loss 36.573280334472656 Time 101.93064403533936
2024-08-13 14:30:38,887 [INFO] Epoch and iter 0 3199 Loss 28.92774200439453 Time 101.92856669425964
2024-08-13 14:32:20,805 [INFO] Epoch and iter 0 3299 Loss 35.165035247802734 Time 101.91749262809753
2024-08-13 14:34:02,750 [INFO] Epoch and iter 0 3399 Loss 41.5111083984375 Time 101.94406843185425
2024-08-13 14:35:44,688 [INFO] Epoch and iter 0 3499 Loss 34.71453857421875 Time 101.93656253814697
2024-08-13 14:36:51,745 [INFO] Saving this epoch  0 +1
2024-08-13 14:36:52,152 [INFO] Epoch 1 started at Tue Aug 13 14:36:52 2024
2024-08-13 14:37:26,904 [INFO] Epoch and iter 1 33 Loss 43.80543899536133 Time 102.21559977531433
2024-08-13 14:39:08,854 [INFO] Epoch and iter 1 133 Loss 29.054147720336914 Time 101.94912672042847
2024-08-13 14:40:50,949 [INFO] Epoch and iter 1 233 Loss 48.98884963989258 Time 102.09423685073853
2024-08-13 14:42:32,879 [INFO] Epoch and iter 1 333 Loss 41.6416015625 Time 101.92933440208435
2024-08-13 14:44:14,822 [INFO] Epoch and iter 1 433 Loss 42.944549560546875 Time 101.94157242774963
2024-08-13 14:45:56,758 [INFO] Epoch and iter 1 533 Loss 33.911155700683594 Time 101.93519353866577
2024-08-13 14:47:38,708 [INFO] Epoch and iter 1 633 Loss 31.87751007080078 Time 101.94935607910156
2024-08-13 14:49:20,658 [INFO] Epoch and iter 1 733 Loss 47.65612030029297 Time 101.94899344444275
2024-08-13 14:51:02,603 [INFO] Epoch and iter 1 833 Loss 31.656322479248047 Time 101.9441454410553
2024-08-13 14:52:44,561 [INFO] Epoch and iter 1 933 Loss 40.4741096496582 Time 101.95790219306946
2024-08-13 14:54:26,523 [INFO] Epoch and iter 1 1033 Loss 40.263824462890625 Time 101.96058535575867
2024-08-13 14:56:08,478 [INFO] Epoch and iter 1 1133 Loss 36.02345275878906 Time 101.95468640327454
2024-08-13 14:57:50,438 [INFO] Epoch and iter 1 1233 Loss 36.540061950683594 Time 101.95896625518799
2024-08-13 14:59:32,390 [INFO] Epoch and iter 1 1333 Loss 40.632362365722656 Time 101.95076704025269
2024-08-13 15:01:14,336 [INFO] Epoch and iter 1 1433 Loss 31.683917999267578 Time 101.94593787193298
2024-08-13 15:02:56,300 [INFO] Epoch and iter 1 1533 Loss 40.1904296875 Time 101.96259927749634
2024-08-13 15:04:38,277 [INFO] Epoch and iter 1 1633 Loss 36.47252655029297 Time 101.97642350196838
2024-08-13 15:06:20,238 [INFO] Epoch and iter 1 1733 Loss 26.932266235351562 Time 101.95968461036682
2024-08-13 15:08:02,214 [INFO] Epoch and iter 1 1833 Loss 44.0523796081543 Time 101.9758186340332
2024-08-13 15:09:44,180 [INFO] Epoch and iter 1 1933 Loss 31.718595504760742 Time 101.96486258506775
2024-08-13 15:11:26,134 [INFO] Epoch and iter 1 2033 Loss 35.56771469116211 Time 101.95293998718262
2024-08-13 15:13:08,094 [INFO] Epoch and iter 1 2133 Loss 27.14754867553711 Time 101.95878529548645
2024-08-13 15:14:50,051 [INFO] Epoch and iter 1 2233 Loss 34.801597595214844 Time 101.95640563964844
2024-08-13 15:16:32,000 [INFO] Epoch and iter 1 2333 Loss 21.706146240234375 Time 101.94803237915039
2024-08-13 15:18:13,941 [INFO] Epoch and iter 1 2433 Loss 29.899593353271484 Time 101.93995428085327
2024-08-13 15:19:55,884 [INFO] Epoch and iter 1 2533 Loss 26.941511154174805 Time 101.94270181655884
2024-08-13 15:21:37,820 [INFO] Epoch and iter 1 2633 Loss 25.848167419433594 Time 101.93521118164062
2024-08-13 15:23:19,766 [INFO] Epoch and iter 1 2733 Loss 30.964405059814453 Time 101.9452805519104
2024-08-13 15:25:01,714 [INFO] Epoch and iter 1 2833 Loss 47.39521789550781 Time 101.94651198387146
2024-08-13 15:26:43,658 [INFO] Epoch and iter 1 2933 Loss 26.444377899169922 Time 101.94279909133911
2024-08-13 15:28:25,593 [INFO] Epoch and iter 1 3033 Loss 32.64032745361328 Time 101.93463015556335
2024-08-13 15:30:07,540 [INFO] Epoch and iter 1 3133 Loss 40.91958999633789 Time 101.94603300094604
2024-08-13 15:31:49,487 [INFO] Epoch and iter 1 3233 Loss 30.347984313964844 Time 101.94670844078064
2024-08-13 15:33:31,443 [INFO] Epoch and iter 1 3333 Loss 26.07050323486328 Time 101.95457220077515
2024-08-13 15:35:13,395 [INFO] Epoch and iter 1 3433 Loss 35.50153350830078 Time 101.95117783546448
2024-08-13 15:36:55,341 [INFO] Epoch and iter 1 3533 Loss 26.342134475708008 Time 101.94508504867554
2024-08-13 15:37:27,778 [INFO] Saving this epoch  1 +1
2024-08-13 15:37:28,186 [INFO] Plotting the results at epoch 1
2024-08-14 10:58:10,596 [INFO] --------------------------------------------------------------------------------
2024-08-14 10:58:10,597 [INFO] Starting training
2024-08-14 10:58:14,641 [INFO] Wandb initialized with project_name : <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x7f2a4dee80a0>>, run_name : main, 123456
2024-08-14 10:58:15,521 [INFO] Dataset size: 62400
2024-08-14 10:58:15,544 [INFO] Train dataset size: 49920
2024-08-14 10:58:15,544 [INFO] Test dataset size: 12480
2024-08-14 10:58:15,544 [INFO] Batch size: 14
2024-08-14 10:58:15,544 [INFO] Batches in train: 3565.714285714286
2024-08-14 10:58:15,544 [INFO] Batches in test: 891.4285714285714
2024-08-14 10:58:20,447 [INFO] Before compilation and loading of slstm.
2024-08-14 10:58:20,487 [INFO] After compilation and loading of slstm.
2024-08-14 10:58:23,466 [INFO] Before compilation and loading of slstm.
2024-08-14 10:58:23,486 [INFO] After compilation and loading of slstm.
2024-08-14 10:58:23,693 [INFO] Model loaded
2024-08-14 10:58:23,695 [INFO] --------------------------------------------------
Model(
  (encoder): XL_encoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (xlstm_point_stack): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=128, out_features=384, bias=False)
            (proj_down): Linear(in_features=192, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=128, out_features=512, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=768, out_features=4, bias=True)
              (fgate): Linear(in_features=768, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=256, out_features=128, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
    (xlstm_point_stack2): xLSTMBlockStack(
      (blocks): ModuleList(
        (0): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): sLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): sLSTMLayer(
            (conv1d): CausalConv1d(
              (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
            )
            (conv_act_fn): SiLU()
            (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=512, num_heads=4)
            (group_norm): MultiHeadLayerNorm()
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm()
          (ffn): GatedFeedForward(
            (proj_up): Linear(in_features=512, out_features=1408, bias=False)
            (proj_down): Linear(in_features=704, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): mLSTMBlock(
          (xlstm_norm): LayerNorm()
          (xlstm): mLSTMLayer(
            (proj_up): Linear(in_features=512, out_features=2048, bias=False)
            (q_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (k_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (v_proj): LinearHeadwiseExpand(in_features=1024, num_heads=256, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
            (conv1d): CausalConv1d(
              (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
            )
            (conv_act_fn): SiLU()
            (mlstm_cell): mLSTMCell(
              (igate): Linear(in_features=3072, out_features=4, bias=True)
              (fgate): Linear(in_features=3072, out_features=4, bias=True)
              (outnorm): MultiHeadLayerNorm()
            )
            (ogate_act_fn): SiLU()
            (proj_down): Linear(in_features=1024, out_features=512, bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (post_blocks_norm): LayerNorm()
    )
  )
  (decoder): PCN_decoder(
    (fc1): Linear(in_features=1024, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (fc3): Linear(in_features=1024, out_features=3072, bias=True)
    (conv1): Conv1d(1029, 512, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(512, 3, kernel_size=(1,), stride=(1,))
  )
)
2024-08-14 10:58:23,696 [INFO] Epochs 50
2024-08-14 10:58:23,696 [INFO] Starting training
2024-08-14 10:58:23,696 [INFO] Epoch 0 started at Wed Aug 14 10:58:23 2024
2024-08-14 11:00:06,559 [INFO] Epoch and iter 0 99 Loss 91.23471069335938 Time 102.86328315734863
2024-08-14 11:01:48,546 [INFO] Epoch and iter 0 199 Loss 67.68831634521484 Time 101.98645186424255
2024-08-14 11:03:30,568 [INFO] Epoch and iter 0 299 Loss 70.73561096191406 Time 102.02077722549438
2024-08-14 11:05:12,568 [INFO] Epoch and iter 0 399 Loss 70.53813934326172 Time 101.99909472465515
2024-08-14 11:06:54,571 [INFO] Epoch and iter 0 499 Loss 58.66657257080078 Time 102.0027642250061
2024-08-14 11:08:36,586 [INFO] Epoch and iter 0 599 Loss 74.79400634765625 Time 102.01397466659546
2024-08-14 11:10:18,605 [INFO] Epoch and iter 0 699 Loss 49.680179595947266 Time 102.01829671859741
2024-08-14 11:12:00,617 [INFO] Epoch and iter 0 799 Loss 78.10757446289062 Time 102.01109004020691
2024-08-14 11:13:42,630 [INFO] Epoch and iter 0 899 Loss 60.591064453125 Time 102.01247143745422
2024-08-14 11:15:24,645 [INFO] Epoch and iter 0 999 Loss 64.27145385742188 Time 102.01405692100525
2024-08-14 11:17:06,660 [INFO] Epoch and iter 0 1099 Loss 48.85813903808594 Time 102.01434898376465
2024-08-14 11:18:48,691 [INFO] Epoch and iter 0 1199 Loss 58.482418060302734 Time 102.0301103591919
2024-08-14 11:20:30,675 [INFO] Epoch and iter 0 1299 Loss 56.20613098144531 Time 101.98262786865234
2024-08-14 11:22:12,634 [INFO] Epoch and iter 0 1399 Loss 53.9071044921875 Time 101.95820879936218
2024-08-14 11:23:54,586 [INFO] Epoch and iter 0 1499 Loss 46.973846435546875 Time 101.95204472541809
2024-08-14 11:25:36,536 [INFO] Epoch and iter 0 1599 Loss 49.02619934082031 Time 101.94881772994995
2024-08-14 11:27:18,489 [INFO] Epoch and iter 0 1699 Loss 50.406734466552734 Time 101.95198392868042
2024-08-14 11:29:00,431 [INFO] Epoch and iter 0 1799 Loss 60.22312545776367 Time 101.94063973426819
2024-08-14 11:30:42,367 [INFO] Epoch and iter 0 1899 Loss 46.44807434082031 Time 101.9351556301117
2024-08-14 11:32:24,291 [INFO] Epoch and iter 0 1999 Loss 39.64265060424805 Time 101.9238052368164
2024-08-14 11:34:06,208 [INFO] Epoch and iter 0 2099 Loss 60.890750885009766 Time 101.91639590263367
2024-08-14 11:35:48,145 [INFO] Epoch and iter 0 2199 Loss 62.213314056396484 Time 101.93608927726746
2024-08-14 11:37:30,097 [INFO] Epoch and iter 0 2299 Loss 41.389827728271484 Time 101.95095491409302
2024-08-14 11:39:12,044 [INFO] Epoch and iter 0 2399 Loss 54.495819091796875 Time 101.94708323478699
2024-08-14 11:40:54,002 [INFO] Epoch and iter 0 2499 Loss 39.15877151489258 Time 101.95704531669617
2024-08-14 11:42:35,960 [INFO] Epoch and iter 0 2599 Loss 56.9292106628418 Time 101.95681071281433
2024-08-14 11:44:17,915 [INFO] Epoch and iter 0 2699 Loss 41.859500885009766 Time 101.95490550994873
2024-08-14 11:45:59,872 [INFO] Epoch and iter 0 2799 Loss 49.644134521484375 Time 101.95580291748047
2024-08-14 11:47:41,827 [INFO] Epoch and iter 0 2899 Loss 42.05299377441406 Time 101.95464897155762
2024-08-14 11:49:23,784 [INFO] Epoch and iter 0 2999 Loss 52.903934478759766 Time 101.95660614967346
2024-08-14 11:51:05,738 [INFO] Epoch and iter 0 3099 Loss 44.36815643310547 Time 101.95281648635864
2024-08-14 11:52:47,695 [INFO] Epoch and iter 0 3199 Loss 40.6865348815918 Time 101.95587301254272
2024-08-14 11:54:29,653 [INFO] Epoch and iter 0 3299 Loss 54.662593841552734 Time 101.95724964141846
2024-08-14 11:56:11,639 [INFO] Epoch and iter 0 3399 Loss 38.29925537109375 Time 101.98534774780273
2024-08-14 11:57:53,598 [INFO] Epoch and iter 0 3499 Loss 33.89845275878906 Time 101.95888495445251
2024-08-14 11:59:00,668 [INFO] Saving this epoch  0 +1
2024-08-14 11:59:01,013 [INFO] Epoch 1 started at Wed Aug 14 11:59:01 2024
2024-08-14 11:59:35,793 [INFO] Epoch and iter 1 33 Loss 35.024024963378906 Time 102.19448161125183
2024-08-14 12:01:17,750 [INFO] Epoch and iter 1 133 Loss 47.516265869140625 Time 101.95642042160034
2024-08-14 12:02:59,703 [INFO] Epoch and iter 1 233 Loss 38.37932205200195 Time 101.95166063308716
2024-08-14 12:04:41,665 [INFO] Epoch and iter 1 333 Loss 33.86865997314453 Time 101.96164965629578
2024-08-14 12:06:23,624 [INFO] Epoch and iter 1 433 Loss 34.654972076416016 Time 101.95839023590088
2024-08-14 12:08:05,569 [INFO] Epoch and iter 1 533 Loss 46.76338195800781 Time 101.94407629966736
2024-08-14 12:09:47,561 [INFO] Epoch and iter 1 633 Loss 47.863067626953125 Time 101.99111080169678
2024-08-14 12:11:29,563 [INFO] Epoch and iter 1 733 Loss 43.00064468383789 Time 102.00203824043274
2024-08-14 12:13:11,639 [INFO] Epoch and iter 1 833 Loss 46.897613525390625 Time 102.07535219192505
2024-08-14 12:14:53,721 [INFO] Epoch and iter 1 933 Loss 49.432132720947266 Time 102.08117389678955
2024-08-14 12:16:35,787 [INFO] Epoch and iter 1 1033 Loss 30.251201629638672 Time 102.06480669975281
2024-08-14 12:18:17,861 [INFO] Epoch and iter 1 1133 Loss 36.755069732666016 Time 102.07405924797058
2024-08-14 12:19:59,933 [INFO] Epoch and iter 1 1233 Loss 31.07077407836914 Time 102.07074117660522
2024-08-14 12:21:42,018 [INFO] Epoch and iter 1 1333 Loss 32.50970458984375 Time 102.08458614349365
2024-08-14 12:23:24,137 [INFO] Epoch and iter 1 1433 Loss 34.293983459472656 Time 102.11811780929565
2024-08-14 12:25:06,244 [INFO] Epoch and iter 1 1533 Loss 33.96464538574219 Time 102.10617089271545
2024-08-14 12:26:48,318 [INFO] Epoch and iter 1 1633 Loss 28.768768310546875 Time 102.07346391677856
2024-08-14 12:28:30,426 [INFO] Epoch and iter 1 1733 Loss 31.393993377685547 Time 102.10689687728882
2024-08-14 12:30:12,540 [INFO] Epoch and iter 1 1833 Loss 33.52135467529297 Time 102.11369681358337
2024-08-14 12:31:54,622 [INFO] Epoch and iter 1 1933 Loss 29.24747085571289 Time 102.0819206237793
2024-08-14 12:33:36,674 [INFO] Epoch and iter 1 2033 Loss 34.02199935913086 Time 102.05087494850159
2024-08-14 12:35:18,711 [INFO] Epoch and iter 1 2133 Loss 29.659255981445312 Time 102.03611135482788
2024-08-14 12:37:00,785 [INFO] Epoch and iter 1 2233 Loss 29.877479553222656 Time 102.07397365570068
2024-08-14 12:38:42,869 [INFO] Epoch and iter 1 2333 Loss 27.4619140625 Time 102.08242964744568
2024-08-14 12:40:24,951 [INFO] Epoch and iter 1 2433 Loss 28.20375633239746 Time 102.0816056728363
2024-08-14 12:42:07,040 [INFO] Epoch and iter 1 2533 Loss 36.74506378173828 Time 102.08882236480713
2024-08-14 12:43:49,090 [INFO] Epoch and iter 1 2633 Loss 33.50253677368164 Time 102.04882979393005
2024-08-14 12:45:31,158 [INFO] Epoch and iter 1 2733 Loss 37.981101989746094 Time 102.06740832328796
2024-08-14 12:47:13,216 [INFO] Epoch and iter 1 2833 Loss 27.27644920349121 Time 102.05685925483704
2024-08-14 12:48:55,265 [INFO] Epoch and iter 1 2933 Loss 39.46247100830078 Time 102.047119140625
2024-08-14 12:50:37,344 [INFO] Epoch and iter 1 3033 Loss 36.258365631103516 Time 102.0785870552063
2024-08-14 12:52:19,427 [INFO] Epoch and iter 1 3133 Loss 32.38895034790039 Time 102.08246850967407
2024-08-14 12:54:01,469 [INFO] Epoch and iter 1 3233 Loss 31.006450653076172 Time 102.04130268096924
2024-08-14 12:55:43,500 [INFO] Epoch and iter 1 3333 Loss 32.73881530761719 Time 102.0302472114563
2024-08-14 12:57:25,594 [INFO] Epoch and iter 1 3433 Loss 35.57189178466797 Time 102.09319019317627
2024-08-14 12:59:07,660 [INFO] Epoch and iter 1 3533 Loss 25.922212600708008 Time 102.06542348861694
2024-08-14 12:59:40,136 [INFO] Saving this epoch  1 +1
2024-08-14 12:59:40,525 [INFO] Epoch 2 started at Wed Aug 14 12:59:40 2024
2024-08-14 13:00:50,056 [INFO] Epoch and iter 2 67 Loss 35.891456604003906 Time 102.39591550827026
2024-08-14 13:02:32,140 [INFO] Epoch and iter 2 167 Loss 33.835113525390625 Time 102.08338952064514
2024-08-14 13:04:14,227 [INFO] Epoch and iter 2 267 Loss 29.49191665649414 Time 102.08644199371338
2024-08-14 13:05:56,305 [INFO] Epoch and iter 2 367 Loss 33.485809326171875 Time 102.07732939720154
2024-08-14 13:07:38,401 [INFO] Epoch and iter 2 467 Loss 36.13848876953125 Time 102.09484624862671
2024-08-14 13:09:20,534 [INFO] Epoch and iter 2 567 Loss 22.484333038330078 Time 102.13249158859253
2024-08-14 13:11:02,647 [INFO] Epoch and iter 2 667 Loss 27.577289581298828 Time 102.1124062538147
2024-08-14 13:12:44,757 [INFO] Epoch and iter 2 767 Loss 28.593997955322266 Time 102.10836219787598
2024-08-14 13:14:26,859 [INFO] Epoch and iter 2 867 Loss 37.76945877075195 Time 102.10164403915405
2024-08-14 13:16:08,959 [INFO] Epoch and iter 2 967 Loss 24.151676177978516 Time 102.099196434021
2024-08-14 13:17:51,058 [INFO] Epoch and iter 2 1067 Loss 27.25269317626953 Time 102.09895420074463
2024-08-14 13:19:33,218 [INFO] Epoch and iter 2 1167 Loss 28.568138122558594 Time 102.15946674346924
2024-08-14 13:21:15,373 [INFO] Epoch and iter 2 1267 Loss 26.17003059387207 Time 102.15377759933472
2024-08-14 13:22:57,515 [INFO] Epoch and iter 2 1367 Loss 38.5914306640625 Time 102.14110064506531
2024-08-14 13:24:39,642 [INFO] Epoch and iter 2 1467 Loss 29.131011962890625 Time 102.12600421905518
2024-08-14 13:26:21,797 [INFO] Epoch and iter 2 1567 Loss 32.10568618774414 Time 102.15467309951782
2024-08-14 13:28:03,931 [INFO] Epoch and iter 2 1667 Loss 22.74165916442871 Time 102.13355875015259
2024-08-14 13:29:46,045 [INFO] Epoch and iter 2 1767 Loss 32.20998764038086 Time 102.113356590271
2024-08-14 13:31:28,153 [INFO] Epoch and iter 2 1867 Loss 27.272050857543945 Time 102.1066300868988
2024-08-14 13:33:10,234 [INFO] Epoch and iter 2 1967 Loss 26.56161880493164 Time 102.0804877281189
2024-08-14 13:34:52,314 [INFO] Epoch and iter 2 2067 Loss 41.18853759765625 Time 102.07948541641235
2024-08-14 13:36:34,416 [INFO] Epoch and iter 2 2167 Loss 22.153404235839844 Time 102.10072445869446
2024-08-14 13:38:16,476 [INFO] Epoch and iter 2 2267 Loss 27.999984741210938 Time 102.05930161476135
2024-08-14 13:39:58,513 [INFO] Epoch and iter 2 2367 Loss 33.670413970947266 Time 102.03623509407043
2024-08-14 13:41:40,544 [INFO] Epoch and iter 2 2467 Loss 29.365022659301758 Time 102.03061842918396
2024-08-14 13:43:22,576 [INFO] Epoch and iter 2 2567 Loss 36.4340934753418 Time 102.03099536895752
2024-08-14 13:45:04,608 [INFO] Epoch and iter 2 2667 Loss 42.16902160644531 Time 102.03131294250488
2024-08-14 13:46:46,674 [INFO] Epoch and iter 2 2767 Loss 34.971065521240234 Time 102.06547236442566
2024-08-14 13:48:28,777 [INFO] Epoch and iter 2 2867 Loss 34.999671936035156 Time 102.10216665267944
2024-08-14 13:50:10,854 [INFO] Epoch and iter 2 2967 Loss 28.267364501953125 Time 102.07641291618347
2024-08-14 13:51:52,935 [INFO] Epoch and iter 2 3067 Loss 27.025421142578125 Time 102.0799412727356
2024-08-14 13:53:35,064 [INFO] Epoch and iter 2 3167 Loss 24.407819747924805 Time 102.12776017189026
2024-08-14 13:55:17,203 [INFO] Epoch and iter 2 3267 Loss 26.930665969848633 Time 102.1384105682373
2024-08-14 13:56:59,348 [INFO] Epoch and iter 2 3367 Loss 26.32750129699707 Time 102.14423155784607
2024-08-14 13:58:41,485 [INFO] Epoch and iter 2 3467 Loss 22.123825073242188 Time 102.13648104667664
2024-08-14 14:00:21,362 [INFO] Saving this epoch  2 +1
2024-08-14 14:00:21,761 [INFO] Epoch 3 started at Wed Aug 14 14:00:21 2024
2024-08-14 14:00:23,901 [INFO] Epoch and iter 3 1 Loss 29.890867233276367 Time 102.41516590118408
2024-08-14 14:02:05,861 [INFO] Epoch and iter 3 101 Loss 31.817277908325195 Time 101.95919251441956
2024-08-14 14:03:47,826 [INFO] Epoch and iter 3 201 Loss 32.34129333496094 Time 101.96439528465271
2024-08-14 14:05:29,787 [INFO] Epoch and iter 3 301 Loss 23.135902404785156 Time 101.96048188209534
2024-08-14 14:07:11,747 [INFO] Epoch and iter 3 401 Loss 32.71100997924805 Time 101.95966219902039
2024-08-14 14:08:53,711 [INFO] Epoch and iter 3 501 Loss 24.25200843811035 Time 101.96224117279053
2024-08-14 14:10:35,682 [INFO] Epoch and iter 3 601 Loss 27.287208557128906 Time 101.9706346988678
2024-08-14 14:12:17,667 [INFO] Epoch and iter 3 701 Loss 27.271018981933594 Time 101.98407506942749
2024-08-14 14:13:59,617 [INFO] Epoch and iter 3 801 Loss 26.654924392700195 Time 101.9487624168396
2024-08-14 14:15:41,567 [INFO] Epoch and iter 3 901 Loss 29.40337371826172 Time 101.9496705532074
2024-08-14 14:17:23,540 [INFO] Epoch and iter 3 1001 Loss 28.635793685913086 Time 101.97201919555664
2024-08-14 14:19:05,493 [INFO] Epoch and iter 3 1101 Loss 24.12052345275879 Time 101.95183897018433
2024-08-14 14:20:47,461 [INFO] Epoch and iter 3 1201 Loss 22.081249237060547 Time 101.96710085868835
2024-08-14 14:22:29,424 [INFO] Epoch and iter 3 1301 Loss 37.388572692871094 Time 101.96228456497192
2024-08-14 14:24:11,390 [INFO] Epoch and iter 3 1401 Loss 34.341243743896484 Time 101.9652373790741
2024-08-14 14:25:53,358 [INFO] Epoch and iter 3 1501 Loss 28.84449005126953 Time 101.96692490577698
2024-08-14 14:27:35,323 [INFO] Epoch and iter 3 1601 Loss 18.554523468017578 Time 101.9650182723999
2024-08-14 14:29:17,300 [INFO] Epoch and iter 3 1701 Loss 20.937204360961914 Time 101.97559881210327
2024-08-14 14:30:59,279 [INFO] Epoch and iter 3 1801 Loss 21.948904037475586 Time 101.97819828987122
2024-08-14 14:32:41,252 [INFO] Epoch and iter 3 1901 Loss 33.92354202270508 Time 101.97222971916199
2024-08-14 14:34:23,200 [INFO] Epoch and iter 3 2001 Loss 27.9669189453125 Time 101.9475965499878
2024-08-14 14:36:05,143 [INFO] Epoch and iter 3 2101 Loss 23.937877655029297 Time 101.94267320632935
2024-08-14 14:37:47,065 [INFO] Epoch and iter 3 2201 Loss 25.96049690246582 Time 101.92048025131226
2024-08-14 14:39:28,981 [INFO] Epoch and iter 3 2301 Loss 29.48580551147461 Time 101.91590476036072
2024-08-14 14:41:10,899 [INFO] Epoch and iter 3 2401 Loss 21.089046478271484 Time 101.91721367835999
2024-08-14 14:42:52,806 [INFO] Epoch and iter 3 2501 Loss 26.333236694335938 Time 101.90585517883301
2024-08-14 14:44:34,714 [INFO] Epoch and iter 3 2601 Loss 21.800594329833984 Time 101.90720796585083
2024-08-14 14:46:16,626 [INFO] Epoch and iter 3 2701 Loss 26.378137588500977 Time 101.91097402572632
2024-08-14 14:47:58,545 [INFO] Epoch and iter 3 2801 Loss 30.987319946289062 Time 101.91877889633179
2024-08-14 14:49:40,477 [INFO] Epoch and iter 3 2901 Loss 21.629674911499023 Time 101.93041896820068
2024-08-14 14:51:22,425 [INFO] Epoch and iter 3 3001 Loss 36.62044143676758 Time 101.947336435318
2024-08-14 14:53:04,381 [INFO] Epoch and iter 3 3101 Loss 28.90423011779785 Time 101.95564651489258
2024-08-14 14:54:46,346 [INFO] Epoch and iter 3 3201 Loss 24.465225219726562 Time 101.96401953697205
2024-08-14 14:56:28,348 [INFO] Epoch and iter 3 3301 Loss 31.895057678222656 Time 102.0020182132721
2024-08-14 14:58:10,346 [INFO] Epoch and iter 3 3401 Loss 24.91940689086914 Time 101.99700236320496
2024-08-14 14:59:52,369 [INFO] Epoch and iter 3 3501 Loss 40.80564498901367 Time 102.02247524261475
2024-08-14 15:00:57,459 [INFO] Saving this epoch  3 +1
2024-08-14 15:00:57,864 [INFO] Epoch 4 started at Wed Aug 14 15:00:57 2024
2024-08-14 15:01:34,689 [INFO] Epoch and iter 4 35 Loss 29.143054962158203 Time 102.31926703453064
2024-08-14 15:03:16,671 [INFO] Epoch and iter 4 135 Loss 24.36165428161621 Time 101.9805657863617
2024-08-14 15:04:58,648 [INFO] Epoch and iter 4 235 Loss 22.35062599182129 Time 101.97685670852661
2024-08-14 15:06:40,635 [INFO] Epoch and iter 4 335 Loss 26.05190086364746 Time 101.98610806465149
2024-08-14 15:08:22,611 [INFO] Epoch and iter 4 435 Loss 22.72523307800293 Time 101.97512793540955
2024-08-14 15:10:04,581 [INFO] Epoch and iter 4 535 Loss 23.65300750732422 Time 101.96971702575684
2024-08-14 15:11:46,545 [INFO] Epoch and iter 4 635 Loss 21.74097442626953 Time 101.96342468261719
2024-08-14 15:13:28,530 [INFO] Epoch and iter 4 735 Loss 26.758434295654297 Time 101.98379468917847
2024-08-14 15:15:10,501 [INFO] Epoch and iter 4 835 Loss 27.427616119384766 Time 101.97032308578491
2024-08-14 15:16:52,469 [INFO] Epoch and iter 4 935 Loss 23.535144805908203 Time 101.96764755249023
2024-08-14 15:18:34,454 [INFO] Epoch and iter 4 1035 Loss 19.248435974121094 Time 101.98317098617554
2024-08-14 15:20:16,432 [INFO] Epoch and iter 4 1135 Loss 28.603561401367188 Time 101.97722458839417
2024-08-14 15:21:58,465 [INFO] Epoch and iter 4 1235 Loss 30.59197998046875 Time 102.03313159942627
2024-08-14 15:23:40,488 [INFO] Epoch and iter 4 1335 Loss 25.46540069580078 Time 102.02215886116028
2024-08-14 15:25:22,509 [INFO] Epoch and iter 4 1435 Loss 20.35832405090332 Time 102.02038550376892
2024-08-14 15:27:04,512 [INFO] Epoch and iter 4 1535 Loss 25.331790924072266 Time 102.00194764137268
2024-08-14 15:28:46,535 [INFO] Epoch and iter 4 1635 Loss 22.968894958496094 Time 102.02214121818542
2024-08-14 15:30:28,545 [INFO] Epoch and iter 4 1735 Loss 23.47661590576172 Time 102.00977110862732
2024-08-14 15:32:10,565 [INFO] Epoch and iter 4 1835 Loss 24.849205017089844 Time 102.01943802833557
2024-08-14 15:33:52,531 [INFO] Epoch and iter 4 1935 Loss 23.762754440307617 Time 101.96482968330383
2024-08-14 15:35:34,469 [INFO] Epoch and iter 4 2035 Loss 20.0051212310791 Time 101.93754291534424
2024-08-14 15:37:16,408 [INFO] Epoch and iter 4 2135 Loss 38.208431243896484 Time 101.93829202651978
2024-08-14 15:38:58,338 [INFO] Epoch and iter 4 2235 Loss 29.084421157836914 Time 101.92878675460815
2024-08-14 15:40:40,287 [INFO] Epoch and iter 4 2335 Loss 19.2291202545166 Time 101.94850373268127
2024-08-14 15:42:22,231 [INFO] Epoch and iter 4 2435 Loss 23.7263126373291 Time 101.94280076026917
2024-08-14 15:44:04,169 [INFO] Epoch and iter 4 2535 Loss 28.320491790771484 Time 101.93723034858704
2024-08-14 15:45:46,140 [INFO] Epoch and iter 4 2635 Loss 24.628604888916016 Time 101.97005653381348
2024-08-14 15:47:28,136 [INFO] Epoch and iter 4 2735 Loss 22.75400161743164 Time 101.99601769447327
2024-08-14 15:49:10,158 [INFO] Epoch and iter 4 2835 Loss 26.471166610717773 Time 102.02101230621338
2024-08-14 15:50:52,203 [INFO] Epoch and iter 4 2935 Loss 21.04842758178711 Time 102.04359745979309
2024-08-14 15:52:34,252 [INFO] Epoch and iter 4 3035 Loss 18.505014419555664 Time 102.04844617843628
2024-08-14 15:54:16,320 [INFO] Epoch and iter 4 3135 Loss 17.446544647216797 Time 102.06713461875916
2024-08-14 15:55:58,397 [INFO] Epoch and iter 4 3235 Loss 29.633155822753906 Time 102.07632660865784
2024-08-14 15:57:40,482 [INFO] Epoch and iter 4 3335 Loss 22.524826049804688 Time 102.08446168899536
2024-08-14 15:59:22,522 [INFO] Epoch and iter 4 3435 Loss 28.32615089416504 Time 102.03916358947754
2024-08-14 16:01:04,534 [INFO] Epoch and iter 4 3535 Loss 22.93505859375 Time 102.01102757453918
2024-08-14 16:01:34,941 [INFO] Saving this epoch  4 +1
2024-08-14 16:01:35,341 [INFO] Epoch 5 started at Wed Aug 14 16:01:35 2024
2024-08-14 16:02:46,938 [INFO] Epoch and iter 5 69 Loss 22.836467742919922 Time 102.40349459648132
2024-08-14 16:04:29,062 [INFO] Epoch and iter 5 169 Loss 26.740772247314453 Time 102.12285375595093
2024-08-14 16:06:11,188 [INFO] Epoch and iter 5 269 Loss 28.27141761779785 Time 102.12548565864563
2024-08-14 16:07:53,331 [INFO] Epoch and iter 5 369 Loss 23.070022583007812 Time 102.14267563819885
2024-08-14 16:09:35,451 [INFO] Epoch and iter 5 469 Loss 19.391769409179688 Time 102.11908030509949
2024-08-14 16:11:17,579 [INFO] Epoch and iter 5 569 Loss 23.6092472076416 Time 102.12812542915344
2024-08-14 16:12:59,705 [INFO] Epoch and iter 5 669 Loss 25.539430618286133 Time 102.1247787475586
2024-08-14 16:14:41,827 [INFO] Epoch and iter 5 769 Loss 18.23107147216797 Time 102.1217770576477
2024-08-14 16:16:23,942 [INFO] Epoch and iter 5 869 Loss 27.269149780273438 Time 102.1145453453064
2024-08-14 16:18:06,061 [INFO] Epoch and iter 5 969 Loss 28.589008331298828 Time 102.11801600456238
2024-08-14 16:19:48,178 [INFO] Epoch and iter 5 1069 Loss 22.757600784301758 Time 102.11596465110779
2024-08-14 16:21:30,301 [INFO] Epoch and iter 5 1169 Loss 24.21808624267578 Time 102.12230181694031
2024-08-14 16:23:12,413 [INFO] Epoch and iter 5 1269 Loss 22.287267684936523 Time 102.11176466941833
2024-08-14 16:24:54,523 [INFO] Epoch and iter 5 1369 Loss 26.959754943847656 Time 102.10852289199829
2024-08-14 16:26:36,632 [INFO] Epoch and iter 5 1469 Loss 27.128936767578125 Time 102.10835409164429
2024-08-14 16:28:18,737 [INFO] Epoch and iter 5 1569 Loss 23.475515365600586 Time 102.10409593582153
2024-08-14 16:30:00,848 [INFO] Epoch and iter 5 1669 Loss 26.977432250976562 Time 102.11083698272705
2024-08-14 16:31:42,988 [INFO] Epoch and iter 5 1769 Loss 14.669118881225586 Time 102.1387848854065
2024-08-14 16:33:25,102 [INFO] Epoch and iter 5 1869 Loss 29.235872268676758 Time 102.11290073394775
2024-08-14 16:35:07,233 [INFO] Epoch and iter 5 1969 Loss 21.973398208618164 Time 102.13076972961426
2024-08-14 16:36:49,359 [INFO] Epoch and iter 5 2069 Loss 22.68568992614746 Time 102.1251311302185
2024-08-14 16:38:31,496 [INFO] Epoch and iter 5 2169 Loss 22.149219512939453 Time 102.13626885414124
2024-08-14 16:40:13,632 [INFO] Epoch and iter 5 2269 Loss 19.17034912109375 Time 102.13462090492249
2024-08-14 16:41:55,759 [INFO] Epoch and iter 5 2369 Loss 20.30384635925293 Time 102.12681221961975
2024-08-14 16:43:37,890 [INFO] Epoch and iter 5 2469 Loss 24.292804718017578 Time 102.12986922264099
2024-08-14 16:45:20,003 [INFO] Epoch and iter 5 2569 Loss 24.07790184020996 Time 102.11228084564209
2024-08-14 16:47:02,128 [INFO] Epoch and iter 5 2669 Loss 21.18636703491211 Time 102.12406301498413
2024-08-14 16:48:44,250 [INFO] Epoch and iter 5 2769 Loss 31.297439575195312 Time 102.12071776390076
2024-08-14 16:50:26,381 [INFO] Epoch and iter 5 2869 Loss 23.828296661376953 Time 102.13097286224365
2024-08-14 16:52:08,496 [INFO] Epoch and iter 5 2969 Loss 25.80121612548828 Time 102.11381006240845
2024-08-14 16:53:50,642 [INFO] Epoch and iter 5 3069 Loss 28.47972297668457 Time 102.14486598968506
2024-08-14 16:55:32,777 [INFO] Epoch and iter 5 3169 Loss 21.799901962280273 Time 102.13499474525452
2024-08-14 16:57:14,909 [INFO] Epoch and iter 5 3269 Loss 25.489046096801758 Time 102.13122653961182
2024-08-14 16:58:57,064 [INFO] Epoch and iter 5 3369 Loss 21.654048919677734 Time 102.15413117408752
2024-08-14 17:00:39,197 [INFO] Epoch and iter 5 3469 Loss 22.973976135253906 Time 102.13202238082886
2024-08-14 17:02:17,051 [INFO] Saving this epoch  5 +1
2024-08-14 17:02:17,448 [INFO] Epoch 6 started at Wed Aug 14 17:02:17 2024
2024-08-14 17:02:21,621 [INFO] Epoch and iter 6 3 Loss 24.13398551940918 Time 102.4233751296997
2024-08-14 17:04:03,620 [INFO] Epoch and iter 6 103 Loss 24.46303939819336 Time 101.9984085559845
2024-08-14 17:05:45,622 [INFO] Epoch and iter 6 203 Loss 16.345998764038086 Time 102.00064635276794
2024-08-14 17:07:27,627 [INFO] Epoch and iter 6 303 Loss 18.541622161865234 Time 102.00475764274597
2024-08-14 17:09:09,640 [INFO] Epoch and iter 6 403 Loss 30.308387756347656 Time 102.01225876808167
2024-08-14 17:10:51,673 [INFO] Epoch and iter 6 503 Loss 17.77113151550293 Time 102.03233671188354
2024-08-14 17:12:33,682 [INFO] Epoch and iter 6 603 Loss 25.828109741210938 Time 102.00711369514465
2024-08-14 17:14:15,704 [INFO] Epoch and iter 6 703 Loss 24.18602180480957 Time 102.02129888534546
2024-08-14 17:15:57,720 [INFO] Epoch and iter 6 803 Loss 25.886354446411133 Time 102.01543021202087
2024-08-14 17:17:39,728 [INFO] Epoch and iter 6 903 Loss 23.41169548034668 Time 102.00720357894897
2024-08-14 17:19:21,747 [INFO] Epoch and iter 6 1003 Loss 26.13517951965332 Time 102.01806902885437
2024-08-14 17:21:03,760 [INFO] Epoch and iter 6 1103 Loss 26.5118408203125 Time 102.01284146308899
2024-08-14 17:22:45,776 [INFO] Epoch and iter 6 1203 Loss 25.12306022644043 Time 102.01539015769958
2024-08-14 17:24:27,762 [INFO] Epoch and iter 6 1303 Loss 18.63950538635254 Time 101.98451972007751
2024-08-14 17:26:09,728 [INFO] Epoch and iter 6 1403 Loss 20.699010848999023 Time 101.96508312225342
2024-08-14 17:27:51,697 [INFO] Epoch and iter 6 1503 Loss 23.997188568115234 Time 101.9677619934082
2024-08-14 17:29:33,668 [INFO] Epoch and iter 6 1603 Loss 23.205150604248047 Time 101.97072887420654
2024-08-14 17:31:15,635 [INFO] Epoch and iter 6 1703 Loss 30.07793617248535 Time 101.96590304374695
2024-08-14 17:32:57,596 [INFO] Epoch and iter 6 1803 Loss 19.38556480407715 Time 101.9599039554596
2024-08-14 17:34:39,561 [INFO] Epoch and iter 6 1903 Loss 20.363391876220703 Time 101.96435856819153
2024-08-14 17:36:21,529 [INFO] Epoch and iter 6 2003 Loss 20.833669662475586 Time 101.96735119819641
2024-08-14 17:38:03,513 [INFO] Epoch and iter 6 2103 Loss 20.505685806274414 Time 101.98295617103577
2024-08-14 17:39:45,484 [INFO] Epoch and iter 6 2203 Loss 19.421689987182617 Time 101.9703016281128
2024-08-14 17:41:27,462 [INFO] Epoch and iter 6 2303 Loss 26.08768653869629 Time 101.97739291191101
2024-08-14 17:43:09,438 [INFO] Epoch and iter 6 2403 Loss 21.106067657470703 Time 101.97511339187622
2024-08-14 17:44:51,410 [INFO] Epoch and iter 6 2503 Loss 19.52010726928711 Time 101.97134280204773
2024-08-14 17:46:33,372 [INFO] Epoch and iter 6 2603 Loss 26.02383041381836 Time 101.96118831634521
2024-08-14 17:48:15,324 [INFO] Epoch and iter 6 2703 Loss 21.80109977722168 Time 101.95110273361206
2024-08-14 17:49:57,288 [INFO] Epoch and iter 6 2803 Loss 25.062076568603516 Time 101.9626145362854
2024-08-14 17:51:39,250 [INFO] Epoch and iter 6 2903 Loss 32.792236328125 Time 101.96124172210693
2024-08-14 17:53:21,208 [INFO] Epoch and iter 6 3003 Loss 34.749080657958984 Time 101.9577112197876
2024-08-14 17:55:03,172 [INFO] Epoch and iter 6 3103 Loss 20.062620162963867 Time 101.9629123210907
2024-08-14 17:56:45,167 [INFO] Epoch and iter 6 3203 Loss 20.334209442138672 Time 101.99423170089722
2024-08-14 17:58:27,155 [INFO] Epoch and iter 6 3303 Loss 26.087013244628906 Time 101.98830819129944
2024-08-14 18:00:09,143 [INFO] Epoch and iter 6 3403 Loss 24.164897918701172 Time 101.98714852333069
2024-08-14 18:01:51,119 [INFO] Epoch and iter 6 3503 Loss 20.7813720703125 Time 101.97537660598755
2024-08-14 18:02:54,156 [INFO] Saving this epoch  6 +1
2024-08-14 18:02:54,552 [INFO] Epoch 7 started at Wed Aug 14 18:02:54 2024
2024-08-14 18:03:33,407 [INFO] Epoch and iter 7 37 Loss 20.909957885742188 Time 102.28751420974731
2024-08-14 18:05:15,366 [INFO] Epoch and iter 7 137 Loss 19.050256729125977 Time 101.95777106285095
2024-08-14 18:06:57,344 [INFO] Epoch and iter 7 237 Loss 18.5482177734375 Time 101.97719025611877
2024-08-14 18:08:39,311 [INFO] Epoch and iter 7 337 Loss 19.90281105041504 Time 101.9655749797821
2024-08-14 18:10:21,287 [INFO] Epoch and iter 7 437 Loss 19.918119430541992 Time 101.97539472579956
2024-08-14 18:12:03,249 [INFO] Epoch and iter 7 537 Loss 14.873868942260742 Time 101.96084022521973
2024-08-14 18:13:45,221 [INFO] Epoch and iter 7 637 Loss 18.469846725463867 Time 101.97163677215576
2024-08-14 18:15:27,191 [INFO] Epoch and iter 7 737 Loss 23.628490447998047 Time 101.96952557563782
2024-08-14 18:17:09,159 [INFO] Epoch and iter 7 837 Loss 28.48731231689453 Time 101.96728253364563
2024-08-14 18:18:51,117 [INFO] Epoch and iter 7 937 Loss 19.276443481445312 Time 101.95733642578125
2024-08-14 18:20:33,083 [INFO] Epoch and iter 7 1037 Loss 28.518814086914062 Time 101.96496677398682
2024-08-14 18:22:15,034 [INFO] Epoch and iter 7 1137 Loss 24.63633918762207 Time 101.95011878013611
2024-08-14 18:23:56,994 [INFO] Epoch and iter 7 1237 Loss 22.61798095703125 Time 101.95948195457458
2024-08-14 18:25:38,947 [INFO] Epoch and iter 7 1337 Loss 14.255133628845215 Time 101.95212411880493
2024-08-14 18:27:20,905 [INFO] Epoch and iter 7 1437 Loss 24.242250442504883 Time 101.95660328865051
2024-08-14 18:29:02,857 [INFO] Epoch and iter 7 1537 Loss 19.373313903808594 Time 101.95184564590454
2024-08-14 18:30:44,798 [INFO] Epoch and iter 7 1637 Loss 23.152524948120117 Time 101.9396800994873
2024-08-14 18:32:26,752 [INFO] Epoch and iter 7 1737 Loss 15.473477363586426 Time 101.9528603553772
2024-08-14 18:34:08,698 [INFO] Epoch and iter 7 1837 Loss 18.47010040283203 Time 101.94609045982361
2024-08-14 18:35:50,636 [INFO] Epoch and iter 7 1937 Loss 22.08332633972168 Time 101.93696165084839
2024-08-14 18:37:32,573 [INFO] Epoch and iter 7 2037 Loss 17.746177673339844 Time 101.93589901924133
2024-08-14 18:39:14,513 [INFO] Epoch and iter 7 2137 Loss 23.84591293334961 Time 101.93904399871826
2024-08-14 18:40:56,458 [INFO] Epoch and iter 7 2237 Loss 23.526748657226562 Time 101.94413566589355
2024-08-14 18:42:38,397 [INFO] Epoch and iter 7 2337 Loss 24.827842712402344 Time 101.93792843818665
2024-08-14 18:44:20,318 [INFO] Epoch and iter 7 2437 Loss 26.35485076904297 Time 101.92089533805847
2024-08-14 18:46:02,236 [INFO] Epoch and iter 7 2537 Loss 25.62602424621582 Time 101.91696834564209
2024-08-14 18:47:44,155 [INFO] Epoch and iter 7 2637 Loss 22.494401931762695 Time 101.91746497154236
2024-08-14 18:49:26,069 [INFO] Epoch and iter 7 2737 Loss 29.896732330322266 Time 101.91325306892395
2024-08-14 18:51:08,039 [INFO] Epoch and iter 7 2837 Loss 20.957761764526367 Time 101.96998238563538
2024-08-14 18:52:49,956 [INFO] Epoch and iter 7 2937 Loss 22.759733200073242 Time 101.9154999256134
2024-08-14 18:54:31,851 [INFO] Epoch and iter 7 3037 Loss 18.939834594726562 Time 101.89453029632568
2024-08-14 18:56:13,767 [INFO] Epoch and iter 7 3137 Loss 16.744991302490234 Time 101.91556000709534
2024-08-14 18:57:55,675 [INFO] Epoch and iter 7 3237 Loss 13.771085739135742 Time 101.9067497253418
2024-08-14 18:59:37,580 [INFO] Epoch and iter 7 3337 Loss 16.02056312561035 Time 101.90477561950684
2024-08-14 19:01:19,489 [INFO] Epoch and iter 7 3437 Loss 22.758047103881836 Time 101.90764236450195
2024-08-14 19:03:01,391 [INFO] Epoch and iter 7 3537 Loss 18.740495681762695 Time 101.90101957321167
2024-08-14 19:03:29,724 [INFO] Saving this epoch  7 +1
2024-08-14 19:03:30,133 [INFO] Epoch 8 started at Wed Aug 14 19:03:30 2024
2024-08-14 19:04:43,579 [INFO] Epoch and iter 8 71 Loss 28.438404083251953 Time 102.18710947036743
2024-08-14 19:06:25,440 [INFO] Epoch and iter 8 171 Loss 26.988834381103516 Time 101.8606231212616
2024-08-14 19:08:07,318 [INFO] Epoch and iter 8 271 Loss 19.79322624206543 Time 101.87661814689636
2024-08-14 19:09:49,205 [INFO] Epoch and iter 8 371 Loss 19.736854553222656 Time 101.88638019561768
2024-08-14 19:11:31,091 [INFO] Epoch and iter 8 471 Loss 20.142995834350586 Time 101.88574624061584
2024-08-14 19:13:13,016 [INFO] Epoch and iter 8 571 Loss 23.957509994506836 Time 101.92386054992676
2024-08-14 19:14:54,912 [INFO] Epoch and iter 8 671 Loss 18.530210494995117 Time 101.89494109153748
2024-08-14 19:16:36,817 [INFO] Epoch and iter 8 771 Loss 17.555837631225586 Time 101.90394473075867
2024-08-14 19:18:18,715 [INFO] Epoch and iter 8 871 Loss 20.527128219604492 Time 101.89726185798645
2024-08-14 19:20:00,596 [INFO] Epoch and iter 8 971 Loss 19.153453826904297 Time 101.88076972961426
2024-08-14 19:21:42,492 [INFO] Epoch and iter 8 1071 Loss 20.038909912109375 Time 101.89541864395142
2024-08-14 19:23:24,366 [INFO] Epoch and iter 8 1171 Loss 21.225509643554688 Time 101.87274241447449
2024-08-14 19:25:06,227 [INFO] Epoch and iter 8 1271 Loss 25.84820556640625 Time 101.86084723472595
2024-08-14 19:26:48,088 [INFO] Epoch and iter 8 1371 Loss 18.027664184570312 Time 101.85955381393433
2024-08-14 19:28:29,968 [INFO] Epoch and iter 8 1471 Loss 20.541175842285156 Time 101.87919545173645
2024-08-14 19:30:11,847 [INFO] Epoch and iter 8 1571 Loss 17.044620513916016 Time 101.87828588485718
2024-08-14 19:31:53,742 [INFO] Epoch and iter 8 1671 Loss 29.464839935302734 Time 101.89386105537415
2024-08-14 19:33:35,630 [INFO] Epoch and iter 8 1771 Loss 27.309581756591797 Time 101.88825678825378
2024-08-14 19:35:17,509 [INFO] Epoch and iter 8 1871 Loss 18.84300422668457 Time 101.87749361991882
2024-08-14 19:36:59,414 [INFO] Epoch and iter 8 1971 Loss 17.519025802612305 Time 101.90482258796692
2024-08-14 19:38:41,314 [INFO] Epoch and iter 8 2071 Loss 18.094955444335938 Time 101.89918208122253
2024-08-14 19:40:23,217 [INFO] Epoch and iter 8 2171 Loss 20.95180320739746 Time 101.9027373790741
2024-08-14 19:42:05,121 [INFO] Epoch and iter 8 2271 Loss 21.048030853271484 Time 101.903071641922
2024-08-14 19:43:47,017 [INFO] Epoch and iter 8 2371 Loss 21.37883949279785 Time 101.89475584030151
2024-08-14 19:45:28,917 [INFO] Epoch and iter 8 2471 Loss 23.556243896484375 Time 101.89934825897217
2024-08-14 19:47:10,791 [INFO] Epoch and iter 8 2571 Loss 19.026084899902344 Time 101.8728039264679
2024-08-14 19:48:52,682 [INFO] Epoch and iter 8 2671 Loss 18.10683822631836 Time 101.8901138305664
2024-08-14 19:50:34,579 [INFO] Epoch and iter 8 2771 Loss 19.60893440246582 Time 101.89705038070679
2024-08-14 19:52:16,457 [INFO] Epoch and iter 8 2871 Loss 20.72353744506836 Time 101.87683701515198
2024-08-14 19:53:58,365 [INFO] Epoch and iter 8 2971 Loss 21.976531982421875 Time 101.90735077857971
2024-08-14 19:55:40,302 [INFO] Epoch and iter 8 3071 Loss 24.62837791442871 Time 101.93637704849243
2024-08-14 19:57:22,250 [INFO] Epoch and iter 8 3171 Loss 25.093725204467773 Time 101.9464282989502
2024-08-14 19:59:04,175 [INFO] Epoch and iter 8 3271 Loss 19.41203498840332 Time 101.92443919181824
2024-08-14 20:00:46,242 [INFO] Epoch and iter 8 3371 Loss 21.875179290771484 Time 102.06673097610474
2024-08-14 20:02:28,383 [INFO] Epoch and iter 8 3471 Loss 25.186044692993164 Time 102.1405017375946
2024-08-14 20:04:04,210 [INFO] Saving this epoch  8 +1
2024-08-14 20:04:04,609 [INFO] Epoch 9 started at Wed Aug 14 20:04:04 2024
2024-08-14 20:04:10,848 [INFO] Epoch and iter 9 5 Loss 19.641714096069336 Time 102.4640121459961
2024-08-14 20:05:52,959 [INFO] Epoch and iter 9 105 Loss 29.045024871826172 Time 102.10966491699219
2024-08-14 20:07:35,082 [INFO] Epoch and iter 9 205 Loss 19.839204788208008 Time 102.12246322631836
2024-08-14 20:09:17,216 [INFO] Epoch and iter 9 305 Loss 16.98043441772461 Time 102.13269805908203
2024-08-14 20:10:59,328 [INFO] Epoch and iter 9 405 Loss 23.957916259765625 Time 102.11153316497803
2024-08-14 20:12:41,442 [INFO] Epoch and iter 9 505 Loss 22.20953941345215 Time 102.11297917366028
2024-08-14 20:14:23,553 [INFO] Epoch and iter 9 605 Loss 22.77408218383789 Time 102.11059355735779
2024-08-14 20:16:05,635 [INFO] Epoch and iter 9 705 Loss 16.206226348876953 Time 102.08098888397217
2024-08-14 20:17:47,710 [INFO] Epoch and iter 9 805 Loss 24.40383529663086 Time 102.07429909706116
2024-08-14 20:19:29,778 [INFO] Epoch and iter 9 905 Loss 19.712282180786133 Time 102.06691455841064
2024-08-14 20:21:11,832 [INFO] Epoch and iter 9 1005 Loss 26.985698699951172 Time 102.05320501327515
2024-08-14 20:22:53,894 [INFO] Epoch and iter 9 1105 Loss 25.641571044921875 Time 102.06200289726257
2024-08-14 20:24:35,940 [INFO] Epoch and iter 9 1205 Loss 22.88062286376953 Time 102.04479694366455
2024-08-14 20:26:17,984 [INFO] Epoch and iter 9 1305 Loss 16.01538848876953 Time 102.04379415512085
2024-08-14 20:28:00,043 [INFO] Epoch and iter 9 1405 Loss 17.92379379272461 Time 102.05782222747803
2024-08-14 20:29:42,095 [INFO] Epoch and iter 9 1505 Loss 32.99173355102539 Time 102.05147194862366
2024-08-14 20:31:24,144 [INFO] Epoch and iter 9 1605 Loss 15.006357192993164 Time 102.04868078231812
2024-08-14 20:33:06,180 [INFO] Epoch and iter 9 1705 Loss 17.27202606201172 Time 102.03489351272583
2024-08-14 20:34:48,219 [INFO] Epoch and iter 9 1805 Loss 24.45696258544922 Time 102.03880405426025
2024-08-14 20:36:30,266 [INFO] Epoch and iter 9 1905 Loss 19.279781341552734 Time 102.04676699638367
2024-08-14 20:38:12,300 [INFO] Epoch and iter 9 2005 Loss 23.811532974243164 Time 102.03253865242004
2024-08-14 20:39:54,343 [INFO] Epoch and iter 9 2105 Loss 14.08194351196289 Time 102.04286003112793
2024-08-14 20:41:36,384 [INFO] Epoch and iter 9 2205 Loss 14.308223724365234 Time 102.03937935829163
2024-08-14 20:43:18,410 [INFO] Epoch and iter 9 2305 Loss 16.960006713867188 Time 102.02619814872742
2024-08-14 20:45:00,460 [INFO] Epoch and iter 9 2405 Loss 15.549405097961426 Time 102.04907727241516
2024-08-14 20:46:42,504 [INFO] Epoch and iter 9 2505 Loss 22.633705139160156 Time 102.04281687736511
2024-08-14 20:48:24,543 [INFO] Epoch and iter 9 2605 Loss 20.46686553955078 Time 102.03883910179138
2024-08-14 20:50:06,585 [INFO] Epoch and iter 9 2705 Loss 21.750507354736328 Time 102.04125237464905
2024-08-14 20:51:48,621 [INFO] Epoch and iter 9 2805 Loss 13.740616798400879 Time 102.0351083278656
2024-08-14 20:53:30,651 [INFO] Epoch and iter 9 2905 Loss 24.53131866455078 Time 102.02853655815125
2024-08-14 20:55:12,679 [INFO] Epoch and iter 9 3005 Loss 22.880918502807617 Time 102.02766108512878
2024-08-14 20:56:54,715 [INFO] Epoch and iter 9 3105 Loss 22.474597930908203 Time 102.03491449356079
2024-08-14 20:58:36,762 [INFO] Epoch and iter 9 3205 Loss 19.723125457763672 Time 102.0465497970581
2024-08-14 21:00:18,800 [INFO] Epoch and iter 9 3305 Loss 20.09979820251465 Time 102.03730320930481
2024-08-14 21:02:00,825 [INFO] Epoch and iter 9 3405 Loss 20.015583038330078 Time 102.02417254447937
2024-08-14 21:03:42,865 [INFO] Epoch and iter 9 3505 Loss 16.03684425354004 Time 102.0387933254242
2024-08-14 21:04:43,908 [INFO] Saving this epoch  9 +1
2024-08-14 21:04:44,342 [INFO] Epoch 10 started at Wed Aug 14 21:04:44 2024
2024-08-14 21:05:25,268 [INFO] Epoch and iter 10 39 Loss 19.658361434936523 Time 102.40177941322327
2024-08-14 21:07:07,353 [INFO] Epoch and iter 10 139 Loss 18.258630752563477 Time 102.08370232582092
2024-08-14 21:08:49,454 [INFO] Epoch and iter 10 239 Loss 20.656700134277344 Time 102.10090756416321
2024-08-14 21:10:31,565 [INFO] Epoch and iter 10 339 Loss 21.453872680664062 Time 102.10996603965759
2024-08-14 21:12:13,688 [INFO] Epoch and iter 10 439 Loss 19.231163024902344 Time 102.12289428710938
2024-08-14 21:13:55,823 [INFO] Epoch and iter 10 539 Loss 20.27059555053711 Time 102.1339464187622
2024-08-14 21:15:37,957 [INFO] Epoch and iter 10 639 Loss 25.624969482421875 Time 102.1332585811615
2024-08-14 21:17:20,102 [INFO] Epoch and iter 10 739 Loss 21.92998504638672 Time 102.1445963382721
2024-08-14 21:19:02,257 [INFO] Epoch and iter 10 839 Loss 17.15834617614746 Time 102.15351366996765
2024-08-14 21:20:44,417 [INFO] Epoch and iter 10 939 Loss 19.40044593811035 Time 102.15926170349121
2024-08-14 21:22:26,590 [INFO] Epoch and iter 10 1039 Loss 19.00725746154785 Time 102.17266869544983
2024-08-14 21:24:08,770 [INFO] Epoch and iter 10 1139 Loss 17.673620223999023 Time 102.17952990531921
2024-08-14 21:25:50,952 [INFO] Epoch and iter 10 1239 Loss 21.24250030517578 Time 102.1807599067688
2024-08-14 21:27:33,131 [INFO] Epoch and iter 10 1339 Loss 19.79656219482422 Time 102.17830967903137
2024-08-14 21:29:15,303 [INFO] Epoch and iter 10 1439 Loss 16.53980827331543 Time 102.17059898376465
2024-08-14 21:30:57,488 [INFO] Epoch and iter 10 1539 Loss 18.230924606323242 Time 102.18485474586487
2024-08-14 21:32:39,674 [INFO] Epoch and iter 10 1639 Loss 21.543285369873047 Time 102.18509125709534
2024-08-14 21:34:21,866 [INFO] Epoch and iter 10 1739 Loss 18.226356506347656 Time 102.19122624397278
2024-08-14 21:36:04,059 [INFO] Epoch and iter 10 1839 Loss 23.44902992248535 Time 102.19286775588989
2024-08-14 21:37:46,246 [INFO] Epoch and iter 10 1939 Loss 21.914060592651367 Time 102.18621492385864
2024-08-14 21:39:28,411 [INFO] Epoch and iter 10 2039 Loss 19.118999481201172 Time 102.16391491889954
2024-08-14 21:41:10,545 [INFO] Epoch and iter 10 2139 Loss 18.630746841430664 Time 102.1332015991211
2024-08-14 21:42:52,663 [INFO] Epoch and iter 10 2239 Loss 32.47732925415039 Time 102.11699080467224
2024-08-14 21:44:34,774 [INFO] Epoch and iter 10 2339 Loss 13.248008728027344 Time 102.1103584766388
2024-08-14 21:46:16,885 [INFO] Epoch and iter 10 2439 Loss 20.241413116455078 Time 102.11031436920166
2024-08-14 21:47:59,008 [INFO] Epoch and iter 10 2539 Loss 19.14552116394043 Time 102.12236428260803
2024-08-14 21:49:41,144 [INFO] Epoch and iter 10 2639 Loss 21.232070922851562 Time 102.13451814651489
2024-08-14 21:51:23,267 [INFO] Epoch and iter 10 2739 Loss 23.633651733398438 Time 102.12308740615845
2024-08-14 21:53:05,392 [INFO] Epoch and iter 10 2839 Loss 16.17181396484375 Time 102.12367129325867
2024-08-14 21:54:47,526 [INFO] Epoch and iter 10 2939 Loss 29.39919662475586 Time 102.13294744491577
2024-08-14 21:56:29,653 [INFO] Epoch and iter 10 3039 Loss 17.990863800048828 Time 102.12668657302856
2024-08-14 21:58:11,773 [INFO] Epoch and iter 10 3139 Loss 21.654678344726562 Time 102.11937165260315
2024-08-14 21:59:53,907 [INFO] Epoch and iter 10 3239 Loss 22.499290466308594 Time 102.1330235004425
2024-08-14 22:01:36,037 [INFO] Epoch and iter 10 3339 Loss 22.67237091064453 Time 102.12927484512329
2024-08-14 22:03:18,171 [INFO] Epoch and iter 10 3439 Loss 20.12863540649414 Time 102.13326787948608
2024-08-14 22:05:00,306 [INFO] Epoch and iter 10 3539 Loss 19.929113388061523 Time 102.13370084762573
2024-08-14 22:05:26,676 [INFO] Saving this epoch  10 +1
2024-08-14 22:05:26,748 [INFO] Epoch 11 started at Wed Aug 14 22:05:26 2024
2024-08-14 22:06:42,288 [INFO] Epoch and iter 11 73 Loss 20.937820434570312 Time 101.98138499259949
2024-08-14 22:08:24,284 [INFO] Epoch and iter 11 173 Loss 24.676734924316406 Time 101.99465823173523
2024-08-14 22:10:06,266 [INFO] Epoch and iter 11 273 Loss 23.128799438476562 Time 101.98207926750183
2024-08-14 22:11:48,249 [INFO] Epoch and iter 11 373 Loss 18.44539451599121 Time 101.98139524459839
2024-08-14 22:13:30,236 [INFO] Epoch and iter 11 473 Loss 18.551557540893555 Time 101.98635959625244
2024-08-14 22:15:12,219 [INFO] Epoch and iter 11 573 Loss 20.045047760009766 Time 101.9824550151825
2024-08-14 22:16:54,214 [INFO] Epoch and iter 11 673 Loss 22.107027053833008 Time 101.994131565094
2024-08-14 22:18:36,210 [INFO] Epoch and iter 11 773 Loss 27.664955139160156 Time 101.99539566040039
2024-08-14 22:20:18,200 [INFO] Epoch and iter 11 873 Loss 19.262874603271484 Time 101.98886561393738
2024-08-14 22:22:00,197 [INFO] Epoch and iter 11 973 Loss 23.4693660736084 Time 101.99642014503479
2024-08-14 22:23:42,187 [INFO] Epoch and iter 11 1073 Loss 22.26983642578125 Time 101.98945546150208
2024-08-14 22:25:24,172 [INFO] Epoch and iter 11 1173 Loss 19.101472854614258 Time 101.98379039764404
2024-08-14 22:27:06,164 [INFO] Epoch and iter 11 1273 Loss 15.791879653930664 Time 101.99130630493164
2024-08-14 22:28:48,138 [INFO] Epoch and iter 11 1373 Loss 19.393529891967773 Time 101.97279334068298
2024-08-14 22:30:30,137 [INFO] Epoch and iter 11 1473 Loss 22.532472610473633 Time 101.99827337265015
2024-08-14 22:32:12,145 [INFO] Epoch and iter 11 1573 Loss 17.185420989990234 Time 102.00723505020142
2024-08-14 22:33:54,146 [INFO] Epoch and iter 11 1673 Loss 18.03834342956543 Time 102.0009925365448
2024-08-14 22:35:36,203 [INFO] Epoch and iter 11 1773 Loss 22.767913818359375 Time 102.05583024024963
2024-08-14 22:37:18,274 [INFO] Epoch and iter 11 1873 Loss 20.98272705078125 Time 102.07062530517578
2024-08-14 22:39:00,331 [INFO] Epoch and iter 11 1973 Loss 17.055753707885742 Time 102.05671954154968
2024-08-14 22:40:42,408 [INFO] Epoch and iter 11 2073 Loss 17.743427276611328 Time 102.07579970359802
2024-08-14 22:42:24,481 [INFO] Epoch and iter 11 2173 Loss 17.270971298217773 Time 102.07279920578003
2024-08-14 22:44:06,551 [INFO] Epoch and iter 11 2273 Loss 20.93153953552246 Time 102.06919240951538
2024-08-14 22:45:48,632 [INFO] Epoch and iter 11 2373 Loss 18.041316986083984 Time 102.08102941513062
2024-08-14 22:47:30,709 [INFO] Epoch and iter 11 2473 Loss 23.736614227294922 Time 102.07565784454346
2024-08-14 22:49:12,789 [INFO] Epoch and iter 11 2573 Loss 24.619789123535156 Time 102.0801203250885
2024-08-14 22:50:54,954 [INFO] Epoch and iter 11 2673 Loss 23.292192459106445 Time 102.1644196510315
2024-08-14 22:52:37,124 [INFO] Epoch and iter 11 2773 Loss 28.943973541259766 Time 102.16938805580139
2024-08-14 22:54:19,282 [INFO] Epoch and iter 11 2873 Loss 16.111530303955078 Time 102.15736961364746
2024-08-14 22:56:01,437 [INFO] Epoch and iter 11 2973 Loss 15.097423553466797 Time 102.15345311164856
2024-08-14 22:57:43,577 [INFO] Epoch and iter 11 3073 Loss 21.945287704467773 Time 102.14032244682312
2024-08-14 22:59:25,722 [INFO] Epoch and iter 11 3173 Loss 22.4036808013916 Time 102.1439802646637
2024-08-14 23:01:07,864 [INFO] Epoch and iter 11 3273 Loss 15.798612594604492 Time 102.14096474647522
2024-08-14 23:02:50,003 [INFO] Epoch and iter 11 3373 Loss 22.902210235595703 Time 102.13884091377258
2024-08-14 23:04:32,102 [INFO] Epoch and iter 11 3473 Loss 26.5743408203125 Time 102.09795141220093
2024-08-14 23:06:05,775 [INFO] Saving this epoch  11 +1
2024-08-14 23:06:05,849 [INFO] Epoch 12 started at Wed Aug 14 23:06:05 2024
2024-08-14 23:06:14,085 [INFO] Epoch and iter 12 7 Loss 21.68494415283203 Time 101.98173213005066
2024-08-14 23:07:56,116 [INFO] Epoch and iter 12 107 Loss 23.867290496826172 Time 102.03023934364319
2024-08-14 23:09:38,137 [INFO] Epoch and iter 12 207 Loss 18.70948028564453 Time 102.02110719680786
2024-08-14 23:11:20,158 [INFO] Epoch and iter 12 307 Loss 19.703941345214844 Time 102.01993131637573
2024-08-14 23:13:02,218 [INFO] Epoch and iter 12 407 Loss 18.65442657470703 Time 102.05948972702026
2024-08-14 23:14:44,273 [INFO] Epoch and iter 12 507 Loss 21.146453857421875 Time 102.05429363250732
2024-08-14 23:16:26,341 [INFO] Epoch and iter 12 607 Loss 19.036958694458008 Time 102.06672692298889
2024-08-14 23:18:08,392 [INFO] Epoch and iter 12 707 Loss 17.36308479309082 Time 102.04995107650757
2024-08-14 23:19:50,457 [INFO] Epoch and iter 12 807 Loss 17.77360725402832 Time 102.06508493423462
2024-08-14 23:21:32,515 [INFO] Epoch and iter 12 907 Loss 19.22981071472168 Time 102.05708980560303
2024-08-14 23:23:14,563 [INFO] Epoch and iter 12 1007 Loss 20.41918182373047 Time 102.04705858230591
2024-08-14 23:24:56,623 [INFO] Epoch and iter 12 1107 Loss 14.465932846069336 Time 102.05868864059448
2024-08-14 23:26:38,690 [INFO] Epoch and iter 12 1207 Loss 25.2767391204834 Time 102.06682205200195
2024-08-14 23:28:20,750 [INFO] Epoch and iter 12 1307 Loss 19.593067169189453 Time 102.05887770652771
2024-08-14 23:30:02,841 [INFO] Epoch and iter 12 1407 Loss 18.404743194580078 Time 102.09008431434631
2024-08-14 23:31:44,921 [INFO] Epoch and iter 12 1507 Loss 18.8023624420166 Time 102.0790159702301
2024-08-14 23:33:26,980 [INFO] Epoch and iter 12 1607 Loss 17.169601440429688 Time 102.05850005149841
2024-08-14 23:35:09,024 [INFO] Epoch and iter 12 1707 Loss 20.365257263183594 Time 102.04350328445435
2024-08-14 23:36:51,090 [INFO] Epoch and iter 12 1807 Loss 22.318801879882812 Time 102.06522989273071
2024-08-14 23:38:33,147 [INFO] Epoch and iter 12 1907 Loss 16.918638229370117 Time 102.05649495124817
2024-08-14 23:40:15,221 [INFO] Epoch and iter 12 2007 Loss 23.02646255493164 Time 102.07277703285217
2024-08-14 23:41:57,274 [INFO] Epoch and iter 12 2107 Loss 26.387447357177734 Time 102.0519802570343
2024-08-14 23:43:39,343 [INFO] Epoch and iter 12 2207 Loss 16.32096290588379 Time 102.0689377784729
2024-08-14 23:45:21,396 [INFO] Epoch and iter 12 2307 Loss 17.552082061767578 Time 102.05189633369446
2024-08-14 23:47:03,459 [INFO] Epoch and iter 12 2407 Loss 23.005857467651367 Time 102.06188249588013
2024-08-14 23:48:45,550 [INFO] Epoch and iter 12 2507 Loss 16.20032501220703 Time 102.09029006958008
2024-08-14 23:50:27,622 [INFO] Epoch and iter 12 2607 Loss 15.106595993041992 Time 102.07150983810425
2024-08-14 23:52:09,680 [INFO] Epoch and iter 12 2707 Loss 21.125057220458984 Time 102.05760979652405
2024-08-14 23:53:51,732 [INFO] Epoch and iter 12 2807 Loss 19.261962890625 Time 102.05108785629272
2024-08-14 23:55:33,786 [INFO] Epoch and iter 12 2907 Loss 21.270864486694336 Time 102.05344676971436
2024-08-14 23:57:15,843 [INFO] Epoch and iter 12 3007 Loss 16.87930679321289 Time 102.05558276176453
2024-08-14 23:58:57,907 [INFO] Epoch and iter 12 3107 Loss 21.83194923400879 Time 102.06360387802124
2024-08-15 00:00:39,969 [INFO] Epoch and iter 12 3207 Loss 17.974843978881836 Time 102.06123328208923
2024-08-15 00:02:22,020 [INFO] Epoch and iter 12 3307 Loss 18.986400604248047 Time 102.05027389526367
2024-08-15 00:04:04,069 [INFO] Epoch and iter 12 3407 Loss 18.154403686523438 Time 102.04842138290405
2024-08-15 00:05:46,112 [INFO] Epoch and iter 12 3507 Loss 15.606775283813477 Time 102.04198431968689
2024-08-15 00:06:45,111 [INFO] Saving this epoch  12 +1
2024-08-15 00:06:45,186 [INFO] Epoch 13 started at Thu Aug 15 00:06:45 2024
2024-08-15 00:07:28,137 [INFO] Epoch and iter 13 41 Loss 18.673133850097656 Time 102.02404737472534
2024-08-15 00:09:10,251 [INFO] Epoch and iter 13 141 Loss 17.419368743896484 Time 102.113689661026
2024-08-15 00:10:52,363 [INFO] Epoch and iter 13 241 Loss 21.4945011138916 Time 102.11118459701538
2024-08-15 00:12:34,459 [INFO] Epoch and iter 13 341 Loss 21.043254852294922 Time 102.09568309783936
2024-08-15 00:14:16,527 [INFO] Epoch and iter 13 441 Loss 23.77471351623535 Time 102.06720042228699
2024-08-15 00:15:58,602 [INFO] Epoch and iter 13 541 Loss 15.330425262451172 Time 102.07364320755005
2024-08-15 00:17:40,676 [INFO] Epoch and iter 13 641 Loss 15.701969146728516 Time 102.07353186607361
2024-08-15 00:19:22,739 [INFO] Epoch and iter 13 741 Loss 29.830448150634766 Time 102.06256079673767
2024-08-15 00:21:04,802 [INFO] Epoch and iter 13 841 Loss 16.842945098876953 Time 102.06227707862854
2024-08-15 00:22:46,879 [INFO] Epoch and iter 13 941 Loss 19.101757049560547 Time 102.07616853713989
2024-08-15 00:24:28,949 [INFO] Epoch and iter 13 1041 Loss 22.605274200439453 Time 102.06959390640259
2024-08-15 00:26:11,038 [INFO] Epoch and iter 13 1141 Loss 23.71129608154297 Time 102.08893585205078
2024-08-15 00:27:53,132 [INFO] Epoch and iter 13 1241 Loss 21.20206069946289 Time 102.09289693832397
2024-08-15 00:29:35,216 [INFO] Epoch and iter 13 1341 Loss 17.036293029785156 Time 102.08344888687134
2024-08-15 00:31:17,267 [INFO] Epoch and iter 13 1441 Loss 25.095800399780273 Time 102.05046224594116
2024-08-15 00:32:59,325 [INFO] Epoch and iter 13 1541 Loss 19.198144912719727 Time 102.05712056159973
2024-08-15 00:34:41,399 [INFO] Epoch and iter 13 1641 Loss 18.119583129882812 Time 102.07388305664062
2024-08-15 00:36:23,485 [INFO] Epoch and iter 13 1741 Loss 20.430255889892578 Time 102.0853841304779
2024-08-15 00:38:05,555 [INFO] Epoch and iter 13 1841 Loss 16.950563430786133 Time 102.06937742233276
2024-08-15 00:39:47,633 [INFO] Epoch and iter 13 1941 Loss 21.962448120117188 Time 102.07684564590454
2024-08-15 00:41:29,711 [INFO] Epoch and iter 13 2041 Loss 17.37821388244629 Time 102.07736349105835
2024-08-15 00:43:11,788 [INFO] Epoch and iter 13 2141 Loss 20.002559661865234 Time 102.07610559463501
2024-08-15 00:44:53,870 [INFO] Epoch and iter 13 2241 Loss 22.80548667907715 Time 102.08191156387329
2024-08-15 00:46:35,928 [INFO] Epoch and iter 13 2341 Loss 22.570999145507812 Time 102.05640292167664
2024-08-15 00:48:18,110 [INFO] Epoch and iter 13 2441 Loss 16.220272064208984 Time 102.18135857582092
2024-08-15 00:50:00,268 [INFO] Epoch and iter 13 2541 Loss 17.634319305419922 Time 102.15802550315857
2024-08-15 00:51:42,448 [INFO] Epoch and iter 13 2641 Loss 21.494098663330078 Time 102.1790201663971
2024-08-15 00:53:24,618 [INFO] Epoch and iter 13 2741 Loss 16.59636688232422 Time 102.16905617713928
2024-08-15 00:55:06,776 [INFO] Epoch and iter 13 2841 Loss 16.25992774963379 Time 102.15733528137207
2024-08-15 00:56:48,928 [INFO] Epoch and iter 13 2941 Loss 22.19062042236328 Time 102.1515154838562
2024-08-15 00:58:31,085 [INFO] Epoch and iter 13 3041 Loss 23.235633850097656 Time 102.15633368492126
2024-08-15 01:00:13,264 [INFO] Epoch and iter 13 3141 Loss 20.043155670166016 Time 102.1783013343811
2024-08-15 01:01:55,420 [INFO] Epoch and iter 13 3241 Loss 14.839462280273438 Time 102.15520596504211
2024-08-15 01:03:37,588 [INFO] Epoch and iter 13 3341 Loss 18.766324996948242 Time 102.16680526733398
2024-08-15 01:05:19,766 [INFO] Epoch and iter 13 3441 Loss 19.066883087158203 Time 102.17744660377502
2024-08-15 01:07:01,924 [INFO] Epoch and iter 13 3541 Loss 22.213638305664062 Time 102.15683269500732
2024-08-15 01:07:26,257 [INFO] Saving this epoch  13 +1
2024-08-15 01:07:26,330 [INFO] Epoch 14 started at Thu Aug 15 01:07:26 2024
2024-08-15 01:08:43,950 [INFO] Epoch and iter 14 75 Loss 23.77696990966797 Time 102.02489185333252
2024-08-15 01:10:25,964 [INFO] Epoch and iter 14 175 Loss 13.011219024658203 Time 102.01356101036072
2024-08-15 01:12:07,964 [INFO] Epoch and iter 14 275 Loss 20.62554359436035 Time 101.99915790557861
2024-08-15 01:13:49,971 [INFO] Epoch and iter 14 375 Loss 14.301957130432129 Time 102.00567817687988
2024-08-15 01:15:31,979 [INFO] Epoch and iter 14 475 Loss 20.241275787353516 Time 102.0073914527893
2024-08-15 01:17:14,005 [INFO] Epoch and iter 14 575 Loss 15.20871353149414 Time 102.02497935295105
2024-08-15 01:18:56,026 [INFO] Epoch and iter 14 675 Loss 17.439369201660156 Time 102.02001857757568
2024-08-15 01:20:38,032 [INFO] Epoch and iter 14 775 Loss 17.541982650756836 Time 102.00506234169006
2024-08-15 01:22:20,021 [INFO] Epoch and iter 14 875 Loss 22.959312438964844 Time 101.98850464820862
2024-08-15 01:24:02,026 [INFO] Epoch and iter 14 975 Loss 18.6278076171875 Time 102.00417304039001
2024-08-15 01:25:44,045 [INFO] Epoch and iter 14 1075 Loss 14.020174026489258 Time 102.01856327056885
2024-08-15 01:27:26,064 [INFO] Epoch and iter 14 1175 Loss 16.981075286865234 Time 102.01827931404114
2024-08-15 01:29:08,077 [INFO] Epoch and iter 14 1275 Loss 18.7965030670166 Time 102.01290154457092
2024-08-15 01:30:50,098 [INFO] Epoch and iter 14 1375 Loss 20.07574462890625 Time 102.01955437660217
2024-08-15 01:32:32,105 [INFO] Epoch and iter 14 1475 Loss 19.61007308959961 Time 102.0064492225647
2024-08-15 01:34:14,131 [INFO] Epoch and iter 14 1575 Loss 17.103832244873047 Time 102.02551126480103
2024-08-15 01:35:56,164 [INFO] Epoch and iter 14 1675 Loss 22.58318328857422 Time 102.03246140480042
2024-08-15 01:37:38,190 [INFO] Epoch and iter 14 1775 Loss 21.45852279663086 Time 102.02403712272644
2024-08-15 01:39:20,218 [INFO] Epoch and iter 14 1875 Loss 20.58169174194336 Time 102.02788043022156
2024-08-15 01:41:02,249 [INFO] Epoch and iter 14 1975 Loss 14.966736793518066 Time 102.03049778938293
2024-08-15 01:42:44,280 [INFO] Epoch and iter 14 2075 Loss 20.203020095825195 Time 102.02928066253662
2024-08-15 01:44:26,320 [INFO] Epoch and iter 14 2175 Loss 22.023807525634766 Time 102.03939819335938
2024-08-15 01:46:08,365 [INFO] Epoch and iter 14 2275 Loss 17.7694149017334 Time 102.04496836662292
2024-08-15 01:47:50,385 [INFO] Epoch and iter 14 2375 Loss 22.427330017089844 Time 102.01929044723511
2024-08-15 01:49:32,404 [INFO] Epoch and iter 14 2475 Loss 20.259632110595703 Time 102.01830577850342
2024-08-15 01:51:14,423 [INFO] Epoch and iter 14 2575 Loss 16.129379272460938 Time 102.01780867576599
2024-08-15 01:52:56,444 [INFO] Epoch and iter 14 2675 Loss 13.589363098144531 Time 102.02076625823975
2024-08-15 01:54:38,470 [INFO] Epoch and iter 14 2775 Loss 16.327022552490234 Time 102.02499771118164
2024-08-15 01:56:20,483 [INFO] Epoch and iter 14 2875 Loss 20.227975845336914 Time 102.01191926002502
2024-08-15 01:58:02,506 [INFO] Epoch and iter 14 2975 Loss 20.450502395629883 Time 102.02188348770142
2024-08-15 01:59:44,519 [INFO] Epoch and iter 14 3075 Loss 21.684833526611328 Time 102.01254200935364
2024-08-15 02:01:26,607 [INFO] Epoch and iter 14 3175 Loss 21.456562042236328 Time 102.08756852149963
2024-08-15 02:03:08,785 [INFO] Epoch and iter 14 3275 Loss 17.02695083618164 Time 102.17697620391846
2024-08-15 02:04:50,929 [INFO] Epoch and iter 14 3375 Loss 16.599876403808594 Time 102.1432933807373
2024-08-15 02:06:33,080 [INFO] Epoch and iter 14 3475 Loss 18.884662628173828 Time 102.15089178085327
2024-08-15 02:08:04,846 [INFO] Saving this epoch  14 +1
2024-08-15 02:08:04,919 [INFO] Epoch 15 started at Thu Aug 15 02:08:04 2024
2024-08-15 02:08:15,221 [INFO] Epoch and iter 15 9 Loss 17.644426345825195 Time 102.13982033729553
2024-08-15 02:09:57,400 [INFO] Epoch and iter 15 109 Loss 24.550857543945312 Time 102.17805218696594
2024-08-15 02:11:39,553 [INFO] Epoch and iter 15 209 Loss 14.945924758911133 Time 102.15253472328186
2024-08-15 02:13:21,700 [INFO] Epoch and iter 15 309 Loss 17.46905517578125 Time 102.14640760421753
2024-08-15 02:15:03,842 [INFO] Epoch and iter 15 409 Loss 21.99853515625 Time 102.14073610305786
2024-08-15 02:16:45,982 [INFO] Epoch and iter 15 509 Loss 21.36354637145996 Time 102.14028716087341
2024-08-15 02:18:28,112 [INFO] Epoch and iter 15 609 Loss 16.987369537353516 Time 102.1288948059082
2024-08-15 02:20:10,244 [INFO] Epoch and iter 15 709 Loss 17.88550567626953 Time 102.13104057312012
2024-08-15 02:21:52,362 [INFO] Epoch and iter 15 809 Loss 16.975793838500977 Time 102.11774921417236
2024-08-15 02:23:34,361 [INFO] Epoch and iter 15 909 Loss 18.816129684448242 Time 101.99779105186462
2024-08-15 02:25:16,303 [INFO] Epoch and iter 15 1009 Loss 21.054710388183594 Time 101.94123673439026
2024-08-15 02:26:58,268 [INFO] Epoch and iter 15 1109 Loss 18.329551696777344 Time 101.96450328826904
2024-08-15 02:28:40,248 [INFO] Epoch and iter 15 1209 Loss 13.12862777709961 Time 101.9795298576355
2024-08-15 02:30:22,237 [INFO] Epoch and iter 15 1309 Loss 20.10195541381836 Time 101.98784017562866
2024-08-15 02:32:04,213 [INFO] Epoch and iter 15 1409 Loss 20.851980209350586 Time 101.9751923084259
2024-08-15 02:33:46,194 [INFO] Epoch and iter 15 1509 Loss 18.13768768310547 Time 101.9804298877716
2024-08-15 02:35:28,195 [INFO] Epoch and iter 15 1609 Loss 20.365222930908203 Time 102.00038552284241
2024-08-15 02:37:10,197 [INFO] Epoch and iter 15 1709 Loss 26.149723052978516 Time 102.00032806396484
2024-08-15 02:38:52,195 [INFO] Epoch and iter 15 1809 Loss 20.958091735839844 Time 101.99724054336548
2024-08-15 02:40:34,190 [INFO] Epoch and iter 15 1909 Loss 15.878460884094238 Time 101.99482655525208
2024-08-15 02:42:16,184 [INFO] Epoch and iter 15 2009 Loss 21.530031204223633 Time 101.99316120147705
2024-08-15 02:43:58,189 [INFO] Epoch and iter 15 2109 Loss 15.625431060791016 Time 102.0038206577301
2024-08-15 02:45:40,186 [INFO] Epoch and iter 15 2209 Loss 18.01328468322754 Time 101.99678683280945
2024-08-15 02:47:22,201 [INFO] Epoch and iter 15 2309 Loss 18.3885498046875 Time 102.0133409500122
2024-08-15 02:49:04,205 [INFO] Epoch and iter 15 2409 Loss 18.832975387573242 Time 102.00351977348328
2024-08-15 02:50:46,240 [INFO] Epoch and iter 15 2509 Loss 18.40531349182129 Time 102.03455519676208
2024-08-15 02:52:28,268 [INFO] Epoch and iter 15 2609 Loss 23.243837356567383 Time 102.02656054496765
2024-08-15 02:54:10,266 [INFO] Epoch and iter 15 2709 Loss 14.722166061401367 Time 101.99731040000916
2024-08-15 02:55:52,294 [INFO] Epoch and iter 15 2809 Loss 26.105403900146484 Time 102.02782011032104
2024-08-15 02:57:34,351 [INFO] Epoch and iter 15 2909 Loss 15.499343872070312 Time 102.05585598945618
2024-08-15 02:59:16,365 [INFO] Epoch and iter 15 3009 Loss 20.18954086303711 Time 102.01338315010071
2024-08-15 03:00:58,395 [INFO] Epoch and iter 15 3109 Loss 15.547607421875 Time 102.02969551086426
2024-08-15 03:02:40,425 [INFO] Epoch and iter 15 3209 Loss 24.441789627075195 Time 102.02939081192017
2024-08-15 03:04:22,446 [INFO] Epoch and iter 15 3309 Loss 18.903236389160156 Time 102.02021741867065
2024-08-15 03:06:04,479 [INFO] Epoch and iter 15 3409 Loss 18.62894058227539 Time 102.03151273727417
2024-08-15 03:07:46,529 [INFO] Epoch and iter 15 3509 Loss 17.77894401550293 Time 102.04979944229126
2024-08-15 03:08:43,505 [INFO] Saving this epoch  15 +1
2024-08-15 03:08:43,579 [INFO] Epoch 16 started at Thu Aug 15 03:08:43 2024
2024-08-15 03:09:28,534 [INFO] Epoch and iter 16 43 Loss 15.332494735717773 Time 102.0040853023529
2024-08-15 03:11:10,529 [INFO] Epoch and iter 16 143 Loss 21.856163024902344 Time 101.99357771873474
2024-08-15 03:12:52,525 [INFO] Epoch and iter 16 243 Loss 19.118614196777344 Time 101.99518966674805
2024-08-15 03:14:34,583 [INFO] Epoch and iter 16 343 Loss 24.190574645996094 Time 102.05774807929993
2024-08-15 03:16:16,637 [INFO] Epoch and iter 16 443 Loss 13.858407974243164 Time 102.05278182029724
2024-08-15 03:17:58,678 [INFO] Epoch and iter 16 543 Loss 14.128364562988281 Time 102.04053544998169
2024-08-15 03:19:40,700 [INFO] Epoch and iter 16 643 Loss 17.234411239624023 Time 102.02129244804382
2024-08-15 03:21:22,710 [INFO] Epoch and iter 16 743 Loss 19.713029861450195 Time 102.0086977481842
2024-08-15 03:23:04,730 [INFO] Epoch and iter 16 843 Loss 21.055715560913086 Time 102.01974749565125
2024-08-15 03:24:46,766 [INFO] Epoch and iter 16 943 Loss 17.602405548095703 Time 102.03512835502625
2024-08-15 03:26:28,810 [INFO] Epoch and iter 16 1043 Loss 16.58592987060547 Time 102.04314231872559
2024-08-15 03:28:10,854 [INFO] Epoch and iter 16 1143 Loss 20.347347259521484 Time 102.04319620132446
2024-08-15 03:29:52,886 [INFO] Epoch and iter 16 1243 Loss 12.333593368530273 Time 102.0314826965332
2024-08-15 03:31:34,925 [INFO] Epoch and iter 16 1343 Loss 22.352523803710938 Time 102.03817534446716
2024-08-15 03:33:16,966 [INFO] Epoch and iter 16 1443 Loss 21.279979705810547 Time 102.04018473625183
2024-08-15 03:34:59,011 [INFO] Epoch and iter 16 1543 Loss 22.836442947387695 Time 102.04411578178406
2024-08-15 03:36:41,063 [INFO] Epoch and iter 16 1643 Loss 16.800914764404297 Time 102.05160808563232
2024-08-15 03:38:23,115 [INFO] Epoch and iter 16 1743 Loss 14.428980827331543 Time 102.0512273311615
2024-08-15 03:40:05,157 [INFO] Epoch and iter 16 1843 Loss 20.105899810791016 Time 102.0405740737915
2024-08-15 03:41:47,185 [INFO] Epoch and iter 16 1943 Loss 17.274288177490234 Time 102.0273950099945
2024-08-15 03:43:29,211 [INFO] Epoch and iter 16 2043 Loss 14.466695785522461 Time 102.02564024925232
2024-08-15 03:45:11,258 [INFO] Epoch and iter 16 2143 Loss 15.664793968200684 Time 102.0463798046112
2024-08-15 03:46:53,284 [INFO] Epoch and iter 16 2243 Loss 13.677464485168457 Time 102.02494192123413
2024-08-15 03:48:35,322 [INFO] Epoch and iter 16 2343 Loss 14.108549118041992 Time 102.03752827644348
2024-08-15 03:50:17,332 [INFO] Epoch and iter 16 2443 Loss 13.847042083740234 Time 102.0093605518341
2024-08-15 03:51:59,339 [INFO] Epoch and iter 16 2543 Loss 15.886577606201172 Time 102.00639224052429
2024-08-15 03:53:41,330 [INFO] Epoch and iter 16 2643 Loss 18.33840560913086 Time 101.99055600166321
2024-08-15 03:55:23,339 [INFO] Epoch and iter 16 2743 Loss 14.048140525817871 Time 102.00809741020203
2024-08-15 03:57:05,348 [INFO] Epoch and iter 16 2843 Loss 18.836353302001953 Time 102.00808668136597
2024-08-15 03:58:47,354 [INFO] Epoch and iter 16 2943 Loss 15.317734718322754 Time 102.00535106658936
2024-08-15 04:00:29,358 [INFO] Epoch and iter 16 3043 Loss 23.21596336364746 Time 102.00242471694946
2024-08-15 04:02:11,352 [INFO] Epoch and iter 16 3143 Loss 20.921470642089844 Time 101.99390935897827
2024-08-15 04:03:53,370 [INFO] Epoch and iter 16 3243 Loss 19.105449676513672 Time 102.01652765274048
2024-08-15 04:05:35,397 [INFO] Epoch and iter 16 3343 Loss 20.60467529296875 Time 102.0262222290039
2024-08-15 04:07:17,431 [INFO] Epoch and iter 16 3443 Loss 18.30278778076172 Time 102.03348350524902
2024-08-15 04:08:59,475 [INFO] Epoch and iter 16 3543 Loss 18.59813117980957 Time 102.04310488700867
2024-08-15 04:09:21,737 [INFO] Saving this epoch  16 +1
2024-08-15 04:09:21,809 [INFO] Epoch 17 started at Thu Aug 15 04:09:21 2024
2024-08-15 04:10:41,474 [INFO] Epoch and iter 17 77 Loss 16.049419403076172 Time 101.99840140342712
2024-08-15 04:12:23,502 [INFO] Epoch and iter 17 177 Loss 13.292854309082031 Time 102.02714157104492
2024-08-15 04:14:05,532 [INFO] Epoch and iter 17 277 Loss 14.411075592041016 Time 102.02863574028015
2024-08-15 04:15:47,564 [INFO] Epoch and iter 17 377 Loss 17.728628158569336 Time 102.03228378295898
2024-08-15 04:17:29,610 [INFO] Epoch and iter 17 477 Loss 17.604251861572266 Time 102.04529142379761
2024-08-15 04:19:11,654 [INFO] Epoch and iter 17 577 Loss 20.27484893798828 Time 102.04370427131653
2024-08-15 04:20:53,680 [INFO] Epoch and iter 17 677 Loss 18.62742042541504 Time 102.0246217250824
2024-08-15 04:22:35,724 [INFO] Epoch and iter 17 777 Loss 15.701131820678711 Time 102.04365420341492
2024-08-15 04:24:17,775 [INFO] Epoch and iter 17 877 Loss 22.986888885498047 Time 102.05054521560669
2024-08-15 04:25:59,816 [INFO] Epoch and iter 17 977 Loss 21.715778350830078 Time 102.04072332382202
2024-08-15 04:27:41,855 [INFO] Epoch and iter 17 1077 Loss 16.826770782470703 Time 102.03837490081787
2024-08-15 04:29:23,906 [INFO] Epoch and iter 17 1177 Loss 17.930198669433594 Time 102.05059456825256
2024-08-15 04:31:05,968 [INFO] Epoch and iter 17 1277 Loss 17.508636474609375 Time 102.06078314781189
2024-08-15 04:32:48,017 [INFO] Epoch and iter 17 1377 Loss 16.394033432006836 Time 102.04846596717834
2024-08-15 04:34:30,071 [INFO] Epoch and iter 17 1477 Loss 17.685352325439453 Time 102.0531394481659
2024-08-15 04:36:12,112 [INFO] Epoch and iter 17 1577 Loss 20.10724449157715 Time 102.04099822044373
2024-08-15 04:37:54,152 [INFO] Epoch and iter 17 1677 Loss 14.909646987915039 Time 102.03893446922302
2024-08-15 04:39:36,186 [INFO] Epoch and iter 17 1777 Loss 18.289819717407227 Time 102.0334644317627
2024-08-15 04:41:18,224 [INFO] Epoch and iter 17 1877 Loss 15.899239540100098 Time 102.03658151626587
2024-08-15 04:43:00,270 [INFO] Epoch and iter 17 1977 Loss 16.648658752441406 Time 102.0453679561615
2024-08-15 04:44:42,326 [INFO] Epoch and iter 17 2077 Loss 22.037904739379883 Time 102.05507564544678
2024-08-15 04:46:24,393 [INFO] Epoch and iter 17 2177 Loss 21.816965103149414 Time 102.06675457954407
2024-08-15 04:48:06,464 [INFO] Epoch and iter 17 2277 Loss 20.495054244995117 Time 102.06961560249329
2024-08-15 04:49:48,630 [INFO] Epoch and iter 17 2377 Loss 21.884929656982422 Time 102.1655342578888
2024-08-15 04:51:30,773 [INFO] Epoch and iter 17 2477 Loss 18.621049880981445 Time 102.14235782623291
2024-08-15 04:53:12,894 [INFO] Epoch and iter 17 2577 Loss 15.974794387817383 Time 102.12035989761353
2024-08-15 04:54:54,937 [INFO] Epoch and iter 17 2677 Loss 17.383031845092773 Time 102.04237651824951
2024-08-15 04:56:36,924 [INFO] Epoch and iter 17 2777 Loss 20.115549087524414 Time 101.98576474189758
2024-08-15 04:58:18,976 [INFO] Epoch and iter 17 2877 Loss 14.197404861450195 Time 102.05136775970459
2024-08-15 05:00:01,046 [INFO] Epoch and iter 17 2977 Loss 15.381217956542969 Time 102.06935739517212
2024-08-15 05:01:43,048 [INFO] Epoch and iter 17 3077 Loss 17.292598724365234 Time 102.00174450874329
2024-08-15 05:03:25,055 [INFO] Epoch and iter 17 3177 Loss 13.752120971679688 Time 102.0056676864624
2024-08-15 05:05:07,068 [INFO] Epoch and iter 17 3277 Loss 18.951828002929688 Time 102.01312613487244
2024-08-15 05:06:49,082 [INFO] Epoch and iter 17 3377 Loss 19.649723052978516 Time 102.01320624351501
2024-08-15 05:08:31,100 [INFO] Epoch and iter 17 3477 Loss 15.784088134765625 Time 102.01739501953125
2024-08-15 05:10:00,714 [INFO] Saving this epoch  17 +1
2024-08-15 05:10:00,790 [INFO] Epoch 18 started at Thu Aug 15 05:10:00 2024
2024-08-15 05:10:13,116 [INFO] Epoch and iter 18 11 Loss 17.493587493896484 Time 102.01518416404724
2024-08-15 05:11:55,095 [INFO] Epoch and iter 18 111 Loss 17.04834747314453 Time 101.97782850265503
2024-08-15 05:13:37,083 [INFO] Epoch and iter 18 211 Loss 18.45000457763672 Time 101.98725008964539
2024-08-15 05:15:19,083 [INFO] Epoch and iter 18 311 Loss 12.889690399169922 Time 101.99924230575562
2024-08-15 05:17:01,094 [INFO] Epoch and iter 18 411 Loss 12.686812400817871 Time 102.00982666015625
2024-08-15 05:18:43,097 [INFO] Epoch and iter 18 511 Loss 16.789886474609375 Time 102.00319290161133
2024-08-15 05:20:25,094 [INFO] Epoch and iter 18 611 Loss 16.36875343322754 Time 101.99571967124939
2024-08-15 05:22:07,087 [INFO] Epoch and iter 18 711 Loss 14.641336441040039 Time 101.99177813529968
2024-08-15 05:23:49,086 [INFO] Epoch and iter 18 811 Loss 16.374422073364258 Time 101.99828624725342
2024-08-15 05:25:31,086 [INFO] Epoch and iter 18 911 Loss 18.245908737182617 Time 101.99956679344177
2024-08-15 05:27:13,088 [INFO] Epoch and iter 18 1011 Loss 18.21332550048828 Time 102.00046372413635
2024-08-15 05:28:55,095 [INFO] Epoch and iter 18 1111 Loss 22.161693572998047 Time 102.0068347454071
2024-08-15 05:30:37,089 [INFO] Epoch and iter 18 1211 Loss 17.104976654052734 Time 101.9929666519165
2024-08-15 05:32:19,162 [INFO] Epoch and iter 18 1311 Loss 17.20668601989746 Time 102.0721206665039
2024-08-15 05:34:01,358 [INFO] Epoch and iter 18 1411 Loss 19.518428802490234 Time 102.19534826278687
2024-08-15 05:35:43,515 [INFO] Epoch and iter 18 1511 Loss 20.45874786376953 Time 102.15624380111694
2024-08-15 05:37:25,722 [INFO] Epoch and iter 18 1611 Loss 17.817119598388672 Time 102.20648670196533
2024-08-15 05:39:07,876 [INFO] Epoch and iter 18 1711 Loss 20.155580520629883 Time 102.15304684638977
2024-08-15 05:40:50,029 [INFO] Epoch and iter 18 1811 Loss 22.863235473632812 Time 102.15288996696472
2024-08-15 05:42:32,164 [INFO] Epoch and iter 18 1911 Loss 16.0734920501709 Time 102.13380241394043
2024-08-15 05:44:14,314 [INFO] Epoch and iter 18 2011 Loss 14.668903350830078 Time 102.14867424964905
2024-08-15 05:45:56,470 [INFO] Epoch and iter 18 2111 Loss 21.654966354370117 Time 102.15587449073792
2024-08-15 05:47:38,620 [INFO] Epoch and iter 18 2211 Loss 15.35670280456543 Time 102.1485550403595
2024-08-15 05:49:20,761 [INFO] Epoch and iter 18 2311 Loss 15.050420761108398 Time 102.14090013504028
2024-08-15 05:51:02,894 [INFO] Epoch and iter 18 2411 Loss 17.11195182800293 Time 102.13131594657898
2024-08-15 05:52:45,021 [INFO] Epoch and iter 18 2511 Loss 13.599150657653809 Time 102.12657332420349
2024-08-15 05:54:27,144 [INFO] Epoch and iter 18 2611 Loss 19.24923324584961 Time 102.12220406532288
2024-08-15 05:56:09,267 [INFO] Epoch and iter 18 2711 Loss 18.201391220092773 Time 102.12193465232849
2024-08-15 05:57:51,377 [INFO] Epoch and iter 18 2811 Loss 21.15949058532715 Time 102.10965514183044
2024-08-15 05:59:33,496 [INFO] Epoch and iter 18 2911 Loss 14.299460411071777 Time 102.11855602264404
2024-08-15 06:01:15,608 [INFO] Epoch and iter 18 3011 Loss 17.678173065185547 Time 102.11093235015869
2024-08-15 06:02:57,719 [INFO] Epoch and iter 18 3111 Loss 18.089447021484375 Time 102.11046552658081
2024-08-15 06:04:39,834 [INFO] Epoch and iter 18 3211 Loss 18.71019744873047 Time 102.1141369342804
2024-08-15 06:06:21,957 [INFO] Epoch and iter 18 3311 Loss 17.79244613647461 Time 102.12174940109253
2024-08-15 06:08:04,076 [INFO] Epoch and iter 18 3411 Loss 22.387195587158203 Time 102.11856245994568
2024-08-15 06:09:46,218 [INFO] Epoch and iter 18 3511 Loss 23.713603973388672 Time 102.14108657836914
2024-08-15 06:10:41,182 [INFO] Saving this epoch  18 +1
2024-08-15 06:10:41,255 [INFO] Epoch 19 started at Thu Aug 15 06:10:41 2024
2024-08-15 06:11:28,233 [INFO] Epoch and iter 19 45 Loss 16.273391723632812 Time 102.01420021057129
2024-08-15 06:13:10,312 [INFO] Epoch and iter 19 145 Loss 17.23269271850586 Time 102.07827472686768
2024-08-15 06:14:52,372 [INFO] Epoch and iter 19 245 Loss 20.711872100830078 Time 102.05924677848816
2024-08-15 06:16:34,427 [INFO] Epoch and iter 19 345 Loss 24.679153442382812 Time 102.05417346954346
2024-08-15 06:18:16,496 [INFO] Epoch and iter 19 445 Loss 14.97371768951416 Time 102.06738233566284
2024-08-15 06:19:58,546 [INFO] Epoch and iter 19 545 Loss 20.079641342163086 Time 102.04982829093933
2024-08-15 06:21:40,606 [INFO] Epoch and iter 19 645 Loss 16.937923431396484 Time 102.05917692184448
2024-08-15 06:23:22,690 [INFO] Epoch and iter 19 745 Loss 23.942781448364258 Time 102.08345699310303
2024-08-15 06:25:04,772 [INFO] Epoch and iter 19 845 Loss 15.28448486328125 Time 102.08094310760498
2024-08-15 06:26:46,853 [INFO] Epoch and iter 19 945 Loss 18.344995498657227 Time 102.08126306533813
2024-08-15 06:28:28,924 [INFO] Epoch and iter 19 1045 Loss 15.452940940856934 Time 102.0692491531372
2024-08-15 06:30:11,018 [INFO] Epoch and iter 19 1145 Loss 13.459603309631348 Time 102.09342288970947
2024-08-15 06:31:53,072 [INFO] Epoch and iter 19 1245 Loss 16.87140655517578 Time 102.05401062965393
2024-08-15 06:33:35,127 [INFO] Epoch and iter 19 1345 Loss 19.166790008544922 Time 102.05339694023132
2024-08-15 06:35:17,222 [INFO] Epoch and iter 19 1445 Loss 13.324219703674316 Time 102.09448170661926
2024-08-15 06:36:59,311 [INFO] Epoch and iter 19 1545 Loss 14.971986770629883 Time 102.08846759796143
2024-08-15 06:38:41,369 [INFO] Epoch and iter 19 1645 Loss 14.9185791015625 Time 102.05771064758301
2024-08-15 06:40:23,410 [INFO] Epoch and iter 19 1745 Loss 14.738285064697266 Time 102.03999090194702
2024-08-15 06:42:05,463 [INFO] Epoch and iter 19 1845 Loss 18.567794799804688 Time 102.05184888839722
2024-08-15 06:43:47,539 [INFO] Epoch and iter 19 1945 Loss 17.745145797729492 Time 102.07570314407349
2024-08-15 06:45:29,602 [INFO] Epoch and iter 19 2045 Loss 17.43299102783203 Time 102.06272792816162
2024-08-15 06:47:11,667 [INFO] Epoch and iter 19 2145 Loss 17.551809310913086 Time 102.0636146068573
2024-08-15 06:48:53,739 [INFO] Epoch and iter 19 2245 Loss 15.734746932983398 Time 102.07132148742676
2024-08-15 06:50:35,818 [INFO] Epoch and iter 19 2345 Loss 17.5401611328125 Time 102.07878994941711
2024-08-15 06:52:17,871 [INFO] Epoch and iter 19 2445 Loss 18.461259841918945 Time 102.05186772346497
2024-08-15 06:53:59,939 [INFO] Epoch and iter 19 2545 Loss 24.800537109375 Time 102.06716322898865
2024-08-15 06:55:42,019 [INFO] Epoch and iter 19 2645 Loss 18.59943389892578 Time 102.08009123802185
2024-08-15 06:57:24,087 [INFO] Epoch and iter 19 2745 Loss 16.58181381225586 Time 102.06653714179993
2024-08-15 06:59:06,149 [INFO] Epoch and iter 19 2845 Loss 20.493118286132812 Time 102.06192827224731
2024-08-15 07:00:48,218 [INFO] Epoch and iter 19 2945 Loss 17.00896644592285 Time 102.06853294372559
2024-08-15 07:02:30,301 [INFO] Epoch and iter 19 3045 Loss 19.552087783813477 Time 102.08238291740417
2024-08-15 07:04:12,385 [INFO] Epoch and iter 19 3145 Loss 15.44990348815918 Time 102.08196878433228
2024-08-15 07:05:54,434 [INFO] Epoch and iter 19 3245 Loss 14.15049934387207 Time 102.04854774475098
2024-08-15 07:07:36,499 [INFO] Epoch and iter 19 3345 Loss 19.262535095214844 Time 102.06444144248962
2024-08-15 07:09:18,581 [INFO] Epoch and iter 19 3445 Loss 12.441817283630371 Time 102.08180236816406
2024-08-15 07:11:00,646 [INFO] Epoch and iter 19 3545 Loss 19.898862838745117 Time 102.06342315673828
2024-08-15 07:11:20,872 [INFO] Saving this epoch  19 +1
2024-08-15 07:11:20,945 [INFO] Epoch 20 started at Thu Aug 15 07:11:20 2024
2024-08-15 07:12:42,674 [INFO] Epoch and iter 20 79 Loss 26.6942138671875 Time 102.0269501209259
2024-08-15 07:14:24,737 [INFO] Epoch and iter 20 179 Loss 14.313081741333008 Time 102.06211566925049
2024-08-15 07:16:06,789 [INFO] Epoch and iter 20 279 Loss 21.009178161621094 Time 102.05129194259644
2024-08-15 07:17:48,837 [INFO] Epoch and iter 20 379 Loss 15.673776626586914 Time 102.04800415039062
2024-08-15 07:19:30,928 [INFO] Epoch and iter 20 479 Loss 16.08518409729004 Time 102.08990979194641
2024-08-15 07:21:13,002 [INFO] Epoch and iter 20 579 Loss 15.966084480285645 Time 102.07167863845825
2024-08-15 07:22:55,031 [INFO] Epoch and iter 20 679 Loss 15.83818244934082 Time 102.02833318710327
2024-08-15 07:24:37,128 [INFO] Epoch and iter 20 779 Loss 19.779088973999023 Time 102.09647417068481
2024-08-15 07:26:19,194 [INFO] Epoch and iter 20 879 Loss 14.097509384155273 Time 102.06482934951782
2024-08-15 07:28:01,230 [INFO] Epoch and iter 20 979 Loss 19.020185470581055 Time 102.03571629524231
2024-08-15 07:29:43,308 [INFO] Epoch and iter 20 1079 Loss 15.382238388061523 Time 102.07685160636902
2024-08-15 07:31:25,430 [INFO] Epoch and iter 20 1179 Loss 17.181365966796875 Time 102.1207869052887
2024-08-15 07:33:07,540 [INFO] Epoch and iter 20 1279 Loss 19.318944931030273 Time 102.10926628112793
2024-08-15 07:34:49,639 [INFO] Epoch and iter 20 1379 Loss 18.171558380126953 Time 102.09868216514587
2024-08-15 07:36:31,777 [INFO] Epoch and iter 20 1479 Loss 14.173166275024414 Time 102.1377341747284
2024-08-15 07:38:13,861 [INFO] Epoch and iter 20 1579 Loss 16.468299865722656 Time 102.08291292190552
2024-08-15 07:39:55,982 [INFO] Epoch and iter 20 1679 Loss 14.349607467651367 Time 102.12055659294128
2024-08-15 07:41:38,084 [INFO] Epoch and iter 20 1779 Loss 21.56003189086914 Time 102.10111784934998
2024-08-15 07:43:20,216 [INFO] Epoch and iter 20 1879 Loss 20.153175354003906 Time 102.13156318664551
2024-08-15 07:45:02,339 [INFO] Epoch and iter 20 1979 Loss 14.492010116577148 Time 102.12144947052002
2024-08-15 07:46:44,452 [INFO] Epoch and iter 20 2079 Loss 17.229602813720703 Time 102.11312508583069
2024-08-15 07:48:26,570 [INFO] Epoch and iter 20 2179 Loss 13.848381042480469 Time 102.1172239780426
2024-08-15 07:50:08,692 [INFO] Epoch and iter 20 2279 Loss 23.4835262298584 Time 102.12081956863403
2024-08-15 07:51:50,787 [INFO] Epoch and iter 20 2379 Loss 21.408845901489258 Time 102.09411430358887
2024-08-15 07:53:32,904 [INFO] Epoch and iter 20 2479 Loss 14.99957275390625 Time 102.11716365814209
2024-08-15 07:55:15,084 [INFO] Epoch and iter 20 2579 Loss 21.17503547668457 Time 102.17837023735046
2024-08-15 07:56:57,262 [INFO] Epoch and iter 20 2679 Loss 18.7745361328125 Time 102.17733097076416
2024-08-15 07:58:39,428 [INFO] Epoch and iter 20 2779 Loss 18.933778762817383 Time 102.16570997238159
2024-08-15 08:00:21,583 [INFO] Epoch and iter 20 2879 Loss 18.57927131652832 Time 102.15411138534546
2024-08-15 08:02:03,728 [INFO] Epoch and iter 20 2979 Loss 19.168344497680664 Time 102.14380431175232
2024-08-15 08:03:45,872 [INFO] Epoch and iter 20 3079 Loss 15.918214797973633 Time 102.14333128929138
2024-08-15 08:05:28,015 [INFO] Epoch and iter 20 3179 Loss 17.958988189697266 Time 102.14233422279358
2024-08-15 08:07:10,144 [INFO] Epoch and iter 20 3279 Loss 18.153657913208008 Time 102.12893438339233
2024-08-15 08:08:52,263 [INFO] Epoch and iter 20 3379 Loss 18.363269805908203 Time 102.11819052696228
2024-08-15 08:10:34,398 [INFO] Epoch and iter 20 3479 Loss 16.59116554260254 Time 102.13363814353943
2024-08-15 08:12:02,030 [INFO] Saving this epoch  20 +1
2024-08-15 08:12:02,103 [INFO] Epoch 21 started at Thu Aug 15 08:12:02 2024
2024-08-15 08:12:16,480 [INFO] Epoch and iter 21 13 Loss 20.775876998901367 Time 102.08211708068848
2024-08-15 08:13:58,616 [INFO] Epoch and iter 21 113 Loss 18.134845733642578 Time 102.13477897644043
2024-08-15 08:15:40,740 [INFO] Epoch and iter 21 213 Loss 19.315460205078125 Time 102.12275767326355
2024-08-15 08:17:22,878 [INFO] Epoch and iter 21 313 Loss 18.87230682373047 Time 102.13772058486938
2024-08-15 08:19:05,002 [INFO] Epoch and iter 21 413 Loss 13.725053787231445 Time 102.12381196022034
2024-08-15 08:20:47,129 [INFO] Epoch and iter 21 513 Loss 19.43061065673828 Time 102.12589716911316
2024-08-15 08:22:29,247 [INFO] Epoch and iter 21 613 Loss 19.72903060913086 Time 102.11709880828857
2024-08-15 08:24:11,373 [INFO] Epoch and iter 21 713 Loss 14.679210662841797 Time 102.12478446960449
2024-08-15 08:25:53,490 [INFO] Epoch and iter 21 813 Loss 18.79060173034668 Time 102.11639833450317
2024-08-15 08:27:35,620 [INFO] Epoch and iter 21 913 Loss 12.96136474609375 Time 102.12964081764221
2024-08-15 08:29:17,743 [INFO] Epoch and iter 21 1013 Loss 16.827423095703125 Time 102.12256240844727
2024-08-15 08:30:59,864 [INFO] Epoch and iter 21 1113 Loss 22.33802604675293 Time 102.12022113800049
2024-08-15 08:32:41,994 [INFO] Epoch and iter 21 1213 Loss 22.160730361938477 Time 102.12906408309937
2024-08-15 08:34:24,128 [INFO] Epoch and iter 21 1313 Loss 17.836807250976562 Time 102.13374829292297
2024-08-15 08:36:06,269 [INFO] Epoch and iter 21 1413 Loss 21.233360290527344 Time 102.139803647995
2024-08-15 08:37:48,420 [INFO] Epoch and iter 21 1513 Loss 22.996681213378906 Time 102.15026354789734
2024-08-15 08:39:30,567 [INFO] Epoch and iter 21 1613 Loss 14.606120109558105 Time 102.14598393440247
2024-08-15 08:41:12,698 [INFO] Epoch and iter 21 1713 Loss 20.740320205688477 Time 102.130206823349
2024-08-15 08:42:54,841 [INFO] Epoch and iter 21 1813 Loss 20.474868774414062 Time 102.14226078987122
2024-08-15 08:44:36,974 [INFO] Epoch and iter 21 1913 Loss 18.902467727661133 Time 102.133061170578
2024-08-15 08:46:19,124 [INFO] Epoch and iter 21 2013 Loss 18.428014755249023 Time 102.14866209030151
2024-08-15 08:48:01,253 [INFO] Epoch and iter 21 2113 Loss 19.944067001342773 Time 102.128751039505
2024-08-15 08:49:43,395 [INFO] Epoch and iter 21 2213 Loss 19.380531311035156 Time 102.1416826248169
2024-08-15 08:51:25,547 [INFO] Epoch and iter 21 2313 Loss 16.661575317382812 Time 102.15050387382507
2024-08-15 08:53:07,695 [INFO] Epoch and iter 21 2413 Loss 20.123289108276367 Time 102.14800000190735
2024-08-15 08:54:49,832 [INFO] Epoch and iter 21 2513 Loss 21.223087310791016 Time 102.13572263717651
2024-08-15 08:56:31,984 [INFO] Epoch and iter 21 2613 Loss 17.71837615966797 Time 102.15179204940796
2024-08-15 08:58:14,125 [INFO] Epoch and iter 21 2713 Loss 17.264118194580078 Time 102.14006400108337
2024-08-15 08:59:56,264 [INFO] Epoch and iter 21 2813 Loss 17.449565887451172 Time 102.13829135894775
2024-08-15 09:01:38,418 [INFO] Epoch and iter 21 2913 Loss 17.946731567382812 Time 102.15337300300598
2024-08-15 09:03:20,561 [INFO] Epoch and iter 21 3013 Loss 17.341915130615234 Time 102.14273643493652
2024-08-15 09:05:02,692 [INFO] Epoch and iter 21 3113 Loss 21.821678161621094 Time 102.130038022995
2024-08-15 09:06:44,829 [INFO] Epoch and iter 21 3213 Loss 15.9356107711792 Time 102.13589239120483
2024-08-15 09:08:26,956 [INFO] Epoch and iter 21 3313 Loss 24.59347152709961 Time 102.12623620033264
2024-08-15 09:10:09,086 [INFO] Epoch and iter 21 3413 Loss 20.088943481445312 Time 102.12972903251648
2024-08-15 09:11:51,216 [INFO] Epoch and iter 21 3513 Loss 19.037511825561523 Time 102.12960696220398
2024-08-15 09:12:44,136 [INFO] Saving this epoch  21 +1
2024-08-15 09:12:44,209 [INFO] Epoch 22 started at Thu Aug 15 09:12:44 2024
2024-08-15 09:13:33,326 [INFO] Epoch and iter 22 47 Loss 14.116165161132812 Time 102.10858631134033
2024-08-15 09:15:15,503 [INFO] Epoch and iter 22 147 Loss 16.18616485595703 Time 102.17634201049805
2024-08-15 09:16:57,675 [INFO] Epoch and iter 22 247 Loss 11.313557624816895 Time 102.17145419120789
2024-08-15 09:18:39,848 [INFO] Epoch and iter 22 347 Loss 17.70953941345215 Time 102.17194247245789
2024-08-15 09:20:22,025 [INFO] Epoch and iter 22 447 Loss 20.932252883911133 Time 102.17639017105103
2024-08-15 09:22:04,208 [INFO] Epoch and iter 22 547 Loss 19.130516052246094 Time 102.18299746513367
2024-08-15 09:23:46,377 [INFO] Epoch and iter 22 647 Loss 14.649372100830078 Time 102.16834688186646
2024-08-15 09:25:28,530 [INFO] Epoch and iter 22 747 Loss 19.14208221435547 Time 102.1519992351532
2024-08-15 09:27:10,650 [INFO] Epoch and iter 22 847 Loss 21.536285400390625 Time 102.11936044692993
2024-08-15 09:28:52,759 [INFO] Epoch and iter 22 947 Loss 17.267797470092773 Time 102.10793209075928
2024-08-15 09:30:34,850 [INFO] Epoch and iter 22 1047 Loss 17.993385314941406 Time 102.09071826934814
2024-08-15 09:32:16,950 [INFO] Epoch and iter 22 1147 Loss 22.770004272460938 Time 102.09918069839478
2024-08-15 09:33:59,053 [INFO] Epoch and iter 22 1247 Loss 22.470203399658203 Time 102.10241365432739
2024-08-15 09:35:41,155 [INFO] Epoch and iter 22 1347 Loss 21.923870086669922 Time 102.10084438323975
2024-08-15 09:37:23,243 [INFO] Epoch and iter 22 1447 Loss 15.567665100097656 Time 102.08736038208008
2024-08-15 09:39:05,324 [INFO] Epoch and iter 22 1547 Loss 14.452192306518555 Time 102.08084106445312
2024-08-15 09:40:47,403 [INFO] Epoch and iter 22 1647 Loss 14.272529602050781 Time 102.07863640785217
2024-08-15 09:42:29,468 [INFO] Epoch and iter 22 1747 Loss 19.25139808654785 Time 102.06356525421143
2024-08-15 09:44:11,545 [INFO] Epoch and iter 22 1847 Loss 18.90194320678711 Time 102.07656002044678
2024-08-15 09:45:53,619 [INFO] Epoch and iter 22 1947 Loss 19.861061096191406 Time 102.07341051101685
2024-08-15 09:47:35,677 [INFO] Epoch and iter 22 2047 Loss 14.801103591918945 Time 102.05764293670654
2024-08-15 09:49:17,746 [INFO] Epoch and iter 22 2147 Loss 14.608097076416016 Time 102.06804037094116
2024-08-15 09:50:59,800 [INFO] Epoch and iter 22 2247 Loss 20.932849884033203 Time 102.05342078208923
2024-08-15 09:52:41,846 [INFO] Epoch and iter 22 2347 Loss 24.88042449951172 Time 102.04484462738037
2024-08-15 09:54:23,880 [INFO] Epoch and iter 22 2447 Loss 20.135784149169922 Time 102.03346228599548
2024-08-15 09:56:05,907 [INFO] Epoch and iter 22 2547 Loss 13.537778854370117 Time 102.02701497077942
2024-08-15 09:57:47,950 [INFO] Epoch and iter 22 2647 Loss 15.087945938110352 Time 102.04163122177124
2024-08-15 09:59:29,996 [INFO] Epoch and iter 22 2747 Loss 16.390029907226562 Time 102.0456817150116
2024-08-15 10:01:12,043 [INFO] Epoch and iter 22 2847 Loss 22.2499942779541 Time 102.04590010643005
2024-08-15 10:02:54,089 [INFO] Epoch and iter 22 2947 Loss 17.2634334564209 Time 102.04595518112183
2024-08-15 10:04:36,117 [INFO] Epoch and iter 22 3047 Loss 17.386234283447266 Time 102.02695274353027
2024-08-15 10:06:18,141 [INFO] Epoch and iter 22 3147 Loss 13.965208053588867 Time 102.02320194244385
2024-08-15 10:08:00,164 [INFO] Epoch and iter 22 3247 Loss 19.616268157958984 Time 102.02206301689148
2024-08-15 10:09:42,191 [INFO] Epoch and iter 22 3347 Loss 17.995407104492188 Time 102.02666878700256
2024-08-15 10:11:24,209 [INFO] Epoch and iter 22 3447 Loss 17.889257431030273 Time 102.01720309257507
2024-08-15 10:13:06,224 [INFO] Epoch and iter 22 3547 Loss 21.283451080322266 Time 102.0141932964325
2024-08-15 10:13:24,405 [INFO] Saving this epoch  22 +1
2024-08-15 10:13:24,478 [INFO] Epoch 23 started at Thu Aug 15 10:13:24 2024
2024-08-15 10:14:48,260 [INFO] Epoch and iter 23 81 Loss 14.813690185546875 Time 102.03517627716064
2024-08-15 10:16:30,361 [INFO] Epoch and iter 23 181 Loss 16.857648849487305 Time 102.10019445419312
2024-08-15 10:18:12,439 [INFO] Epoch and iter 23 281 Loss 18.636930465698242 Time 102.07818865776062
2024-08-15 10:19:54,520 [INFO] Epoch and iter 23 381 Loss 18.834728240966797 Time 102.07976269721985
2024-08-15 10:21:36,613 [INFO] Epoch and iter 23 481 Loss 17.907508850097656 Time 102.09274387359619
2024-08-15 10:23:18,706 [INFO] Epoch and iter 23 581 Loss 17.99673843383789 Time 102.0915789604187
2024-08-15 10:25:00,802 [INFO] Epoch and iter 23 681 Loss 19.203880310058594 Time 102.09588623046875
2024-08-15 10:26:42,902 [INFO] Epoch and iter 23 781 Loss 19.163545608520508 Time 102.09950089454651
2024-08-15 10:28:25,004 [INFO] Epoch and iter 23 881 Loss 16.209320068359375 Time 102.10107398033142
2024-08-15 10:30:07,107 [INFO] Epoch and iter 23 981 Loss 16.969783782958984 Time 102.10176587104797
2024-08-15 10:31:49,215 [INFO] Epoch and iter 23 1081 Loss 12.141197204589844 Time 102.10727190971375
2024-08-15 10:33:31,321 [INFO] Epoch and iter 23 1181 Loss 18.227434158325195 Time 102.10550498962402
2024-08-15 10:35:13,434 [INFO] Epoch and iter 23 1281 Loss 14.3976411819458 Time 102.11212754249573
2024-08-15 10:36:55,524 [INFO] Epoch and iter 23 1381 Loss 15.115291595458984 Time 102.08975028991699
2024-08-15 10:38:37,620 [INFO] Epoch and iter 23 1481 Loss 15.914834022521973 Time 102.09537887573242
2024-08-15 10:40:19,722 [INFO] Epoch and iter 23 1581 Loss 17.297252655029297 Time 102.10089325904846
2024-08-15 10:42:01,803 [INFO] Epoch and iter 23 1681 Loss 15.21368408203125 Time 102.08061122894287
2024-08-15 10:43:43,890 [INFO] Epoch and iter 23 1781 Loss 16.136865615844727 Time 102.08622980117798
2024-08-15 10:45:25,989 [INFO] Epoch and iter 23 1881 Loss 20.90953826904297 Time 102.0982608795166
2024-08-15 10:47:08,089 [INFO] Epoch and iter 23 1981 Loss 15.989702224731445 Time 102.09914422035217
2024-08-15 10:48:50,178 [INFO] Epoch and iter 23 2081 Loss 21.31940460205078 Time 102.08855986595154
2024-08-15 10:50:32,284 [INFO] Epoch and iter 23 2181 Loss 19.086877822875977 Time 102.10518765449524
2024-08-15 10:52:14,397 [INFO] Epoch and iter 23 2281 Loss 15.712051391601562 Time 102.11230850219727
2024-08-15 10:53:56,503 [INFO] Epoch and iter 23 2381 Loss 18.084789276123047 Time 102.10596799850464
2024-08-15 10:55:38,611 [INFO] Epoch and iter 23 2481 Loss 17.80995750427246 Time 102.10700464248657
2024-08-15 10:57:20,711 [INFO] Epoch and iter 23 2581 Loss 14.627349853515625 Time 102.09935283660889
2024-08-15 10:59:02,826 [INFO] Epoch and iter 23 2681 Loss 14.420095443725586 Time 102.11438250541687
2024-08-15 11:00:44,936 [INFO] Epoch and iter 23 2781 Loss 17.124752044677734 Time 102.10906386375427
2024-08-15 11:02:27,046 [INFO] Epoch and iter 23 2881 Loss 15.654024124145508 Time 102.1096715927124
2024-08-15 11:04:09,146 [INFO] Epoch and iter 23 2981 Loss 12.390066146850586 Time 102.09907937049866
2024-08-15 11:05:51,266 [INFO] Epoch and iter 23 3081 Loss 19.362791061401367 Time 102.11888980865479
2024-08-15 11:07:33,380 [INFO] Epoch and iter 23 3181 Loss 15.775989532470703 Time 102.11404275894165
2024-08-15 11:09:15,506 [INFO] Epoch and iter 23 3281 Loss 17.698944091796875 Time 102.12490963935852
2024-08-15 11:10:57,602 [INFO] Epoch and iter 23 3381 Loss 15.702042579650879 Time 102.09559512138367
2024-08-15 11:12:39,660 [INFO] Epoch and iter 23 3481 Loss 19.8846492767334 Time 102.05760025978088
2024-08-15 11:14:05,177 [INFO] Saving this epoch  23 +1
2024-08-15 11:14:05,248 [INFO] Epoch 24 started at Thu Aug 15 11:14:05 2024
2024-08-15 11:14:21,654 [INFO] Epoch and iter 24 15 Loss 18.190135955810547 Time 101.99305582046509
2024-08-15 11:16:03,712 [INFO] Epoch and iter 24 115 Loss 19.623157501220703 Time 102.05721855163574
2024-08-15 11:17:45,751 [INFO] Epoch and iter 24 215 Loss 21.09386444091797 Time 102.03801083564758
2024-08-15 11:19:27,794 [INFO] Epoch and iter 24 315 Loss 19.048076629638672 Time 102.04195928573608
2024-08-15 11:21:09,823 [INFO] Epoch and iter 24 415 Loss 18.48737144470215 Time 102.02864575386047
2024-08-15 11:22:51,856 [INFO] Epoch and iter 24 515 Loss 17.473276138305664 Time 102.03223705291748
2024-08-15 11:24:33,902 [INFO] Epoch and iter 24 615 Loss 16.95171356201172 Time 102.04602456092834
2024-08-15 11:26:15,940 [INFO] Epoch and iter 24 715 Loss 14.039554595947266 Time 102.03657245635986
2024-08-15 11:27:57,969 [INFO] Epoch and iter 24 815 Loss 22.750465393066406 Time 102.02870726585388
2024-08-15 11:29:40,020 [INFO] Epoch and iter 24 915 Loss 18.337980270385742 Time 102.05057501792908
2024-08-15 11:31:22,069 [INFO] Epoch and iter 24 1015 Loss 16.403491973876953 Time 102.04790711402893
2024-08-15 11:33:04,100 [INFO] Epoch and iter 24 1115 Loss 20.772178649902344 Time 102.03018474578857
2024-08-15 11:34:46,131 [INFO] Epoch and iter 24 1215 Loss 16.493938446044922 Time 102.03065299987793
2024-08-15 11:36:28,178 [INFO] Epoch and iter 24 1315 Loss 19.352346420288086 Time 102.04619312286377
2024-08-15 11:38:10,251 [INFO] Epoch and iter 24 1415 Loss 15.39253044128418 Time 102.0722086429596
2024-08-15 11:39:52,314 [INFO] Epoch and iter 24 1515 Loss 17.395458221435547 Time 102.0620801448822
2024-08-15 11:41:34,367 [INFO] Epoch and iter 24 1615 Loss 18.00503921508789 Time 102.05297303199768
2024-08-15 11:43:16,419 [INFO] Epoch and iter 24 1715 Loss 16.834964752197266 Time 102.0510790348053
2024-08-15 11:44:58,454 [INFO] Epoch and iter 24 1815 Loss 14.942102432250977 Time 102.03437280654907
2024-08-15 11:46:40,625 [INFO] Epoch and iter 24 1915 Loss 15.589975357055664 Time 102.16985368728638
2024-08-15 11:48:22,675 [INFO] Epoch and iter 24 2015 Loss 17.798633575439453 Time 102.0496232509613
2024-08-15 11:50:04,704 [INFO] Epoch and iter 24 2115 Loss 14.289894104003906 Time 102.02881669998169
2024-08-15 11:51:46,738 [INFO] Epoch and iter 24 2215 Loss 19.270952224731445 Time 102.03313302993774
2024-08-15 11:53:28,751 [INFO] Epoch and iter 24 2315 Loss 14.088645935058594 Time 102.01223254203796
2024-08-15 11:55:10,761 [INFO] Epoch and iter 24 2415 Loss 18.926204681396484 Time 102.00970888137817
2024-08-15 11:56:52,786 [INFO] Epoch and iter 24 2515 Loss 23.597423553466797 Time 102.0243182182312
2024-08-15 11:58:34,829 [INFO] Epoch and iter 24 2615 Loss 16.83810806274414 Time 102.04176139831543
2024-08-15 12:00:16,861 [INFO] Epoch and iter 24 2715 Loss 18.981542587280273 Time 102.03150582313538
2024-08-15 12:01:58,897 [INFO] Epoch and iter 24 2815 Loss 14.42947006225586 Time 102.03567624092102
2024-08-15 12:03:40,929 [INFO] Epoch and iter 24 2915 Loss 15.770631790161133 Time 102.03121900558472
2024-08-15 12:05:22,982 [INFO] Epoch and iter 24 3015 Loss 22.850605010986328 Time 102.05238723754883
2024-08-15 12:07:05,025 [INFO] Epoch and iter 24 3115 Loss 17.997779846191406 Time 102.04178309440613
2024-08-15 12:08:47,050 [INFO] Epoch and iter 24 3215 Loss 20.33543586730957 Time 102.02456831932068
2024-08-15 12:10:29,083 [INFO] Epoch and iter 24 3315 Loss 15.23990535736084 Time 102.03224396705627
2024-08-15 12:12:11,121 [INFO] Epoch and iter 24 3415 Loss 17.219968795776367 Time 102.03713607788086
2024-08-15 12:13:53,157 [INFO] Epoch and iter 24 3515 Loss 16.567317962646484 Time 102.03569316864014
2024-08-15 12:14:43,971 [INFO] Saving this epoch  24 +1
2024-08-15 12:14:44,041 [INFO] Epoch 25 started at Thu Aug 15 12:14:44 2024
2024-08-15 12:15:35,138 [INFO] Epoch and iter 25 49 Loss 14.637460708618164 Time 101.98009085655212
2024-08-15 12:17:17,164 [INFO] Epoch and iter 25 149 Loss 17.64456558227539 Time 102.02583909034729
2024-08-15 12:18:59,199 [INFO] Epoch and iter 25 249 Loss 15.721491813659668 Time 102.03350615501404
2024-08-15 12:20:41,226 [INFO] Epoch and iter 25 349 Loss 14.448359489440918 Time 102.02721190452576
2024-08-15 12:22:23,255 [INFO] Epoch and iter 25 449 Loss 14.368106842041016 Time 102.02771067619324
2024-08-15 12:24:05,292 [INFO] Epoch and iter 25 549 Loss 18.824005126953125 Time 102.03654742240906
2024-08-15 12:25:47,324 [INFO] Epoch and iter 25 649 Loss 23.454940795898438 Time 102.03076434135437
2024-08-15 12:27:29,372 [INFO] Epoch and iter 25 749 Loss 16.497236251831055 Time 102.04782438278198
2024-08-15 12:29:11,413 [INFO] Epoch and iter 25 849 Loss 16.12626838684082 Time 102.0405695438385
2024-08-15 12:30:53,444 [INFO] Epoch and iter 25 949 Loss 19.012685775756836 Time 102.03026533126831
2024-08-15 12:32:35,500 [INFO] Epoch and iter 25 1049 Loss 18.658838272094727 Time 102.05461835861206
2024-08-15 12:34:17,536 [INFO] Epoch and iter 25 1149 Loss 20.952861785888672 Time 102.03607940673828
2024-08-15 12:35:59,585 [INFO] Epoch and iter 25 1249 Loss 14.869465827941895 Time 102.04777216911316
2024-08-15 12:37:41,625 [INFO] Epoch and iter 25 1349 Loss 18.073068618774414 Time 102.03985643386841
2024-08-15 12:39:23,684 [INFO] Epoch and iter 25 1449 Loss 16.9198055267334 Time 102.05771613121033
2024-08-15 12:41:05,717 [INFO] Epoch and iter 25 1549 Loss 9.557371139526367 Time 102.03242897987366
2024-08-15 12:42:47,750 [INFO] Epoch and iter 25 1649 Loss 20.12390899658203 Time 102.032475233078
2024-08-15 12:44:29,792 [INFO] Epoch and iter 25 1749 Loss 19.65047836303711 Time 102.0412209033966
2024-08-15 12:46:11,842 [INFO] Epoch and iter 25 1849 Loss 25.22441864013672 Time 102.04952096939087
2024-08-15 12:47:53,878 [INFO] Epoch and iter 25 1949 Loss 17.88463020324707 Time 102.0357174873352
2024-08-15 12:49:35,926 [INFO] Epoch and iter 25 2049 Loss 17.167137145996094 Time 102.04673314094543
2024-08-15 12:51:17,973 [INFO] Epoch and iter 25 2149 Loss 13.32501220703125 Time 102.04637145996094
2024-08-15 12:53:00,026 [INFO] Epoch and iter 25 2249 Loss 15.320146560668945 Time 102.05214595794678
2024-08-15 12:54:42,079 [INFO] Epoch and iter 25 2349 Loss 13.99991226196289 Time 102.05242085456848
2024-08-15 12:56:24,117 [INFO] Epoch and iter 25 2449 Loss 17.634536743164062 Time 102.03734135627747
2024-08-15 12:58:06,171 [INFO] Epoch and iter 25 2549 Loss 21.2191162109375 Time 102.0534918308258
2024-08-15 12:59:48,218 [INFO] Epoch and iter 25 2649 Loss 15.024629592895508 Time 102.04617071151733
2024-08-15 13:01:30,278 [INFO] Epoch and iter 25 2749 Loss 16.796024322509766 Time 102.06003379821777
2024-08-15 13:03:12,322 [INFO] Epoch and iter 25 2849 Loss 16.62211799621582 Time 102.04243159294128
2024-08-15 13:04:54,369 [INFO] Epoch and iter 25 2949 Loss 18.04290199279785 Time 102.04674124717712
2024-08-15 13:06:36,428 [INFO] Epoch and iter 25 3049 Loss 19.924339294433594 Time 102.05823683738708
2024-08-15 13:08:18,479 [INFO] Epoch and iter 25 3149 Loss 15.552785873413086 Time 102.05034399032593
2024-08-15 13:10:00,533 [INFO] Epoch and iter 25 3249 Loss 12.713129043579102 Time 102.05340027809143
2024-08-15 13:11:42,585 [INFO] Epoch and iter 25 3349 Loss 31.477270126342773 Time 102.05128860473633
2024-08-15 13:13:24,634 [INFO] Epoch and iter 25 3449 Loss 21.182491302490234 Time 102.04794788360596
2024-08-15 13:15:06,663 [INFO] Epoch and iter 25 3549 Loss 12.159646987915039 Time 102.02885031700134
2024-08-15 13:15:22,797 [INFO] Saving this epoch  25 +1
2024-08-15 13:15:22,869 [INFO] Epoch 26 started at Thu Aug 15 13:15:22 2024
2024-08-15 13:16:48,684 [INFO] Epoch and iter 26 83 Loss 18.721782684326172 Time 102.02011179924011
2024-08-15 13:18:30,744 [INFO] Epoch and iter 26 183 Loss 17.365047454833984 Time 102.05958318710327
2024-08-15 13:20:12,797 [INFO] Epoch and iter 26 283 Loss 18.43109703063965 Time 102.05225825309753
2024-08-15 13:21:54,839 [INFO] Epoch and iter 26 383 Loss 13.763900756835938 Time 102.04135060310364
2024-08-15 13:23:36,890 [INFO] Epoch and iter 26 483 Loss 16.030799865722656 Time 102.05018663406372
2024-08-15 13:25:18,929 [INFO] Epoch and iter 26 583 Loss 20.505985260009766 Time 102.03856253623962
2024-08-15 13:27:00,980 [INFO] Epoch and iter 26 683 Loss 13.48340129852295 Time 102.05039143562317
2024-08-15 13:28:43,026 [INFO] Epoch and iter 26 783 Loss 22.220775604248047 Time 102.04536294937134
2024-08-15 13:30:25,076 [INFO] Epoch and iter 26 883 Loss 14.695417404174805 Time 102.04850196838379
2024-08-15 13:32:07,111 [INFO] Epoch and iter 26 983 Loss 22.33596420288086 Time 102.03518080711365
2024-08-15 13:33:49,154 [INFO] Epoch and iter 26 1083 Loss 17.810752868652344 Time 102.04231071472168
2024-08-15 13:35:31,181 [INFO] Epoch and iter 26 1183 Loss 17.438190460205078 Time 102.02556371688843
2024-08-15 13:37:13,218 [INFO] Epoch and iter 26 1283 Loss 21.22467803955078 Time 102.03648447990417
2024-08-15 13:38:55,257 [INFO] Epoch and iter 26 1383 Loss 19.137367248535156 Time 102.03807330131531
2024-08-15 13:40:37,282 [INFO] Epoch and iter 26 1483 Loss 17.34635353088379 Time 102.02446842193604
2024-08-15 13:42:19,312 [INFO] Epoch and iter 26 1583 Loss 16.703014373779297 Time 102.02957487106323
2024-08-15 13:44:01,339 [INFO] Epoch and iter 26 1683 Loss 17.881546020507812 Time 102.02689409255981
2024-08-15 13:45:43,381 [INFO] Epoch and iter 26 1783 Loss 15.274250030517578 Time 102.04082107543945
2024-08-15 13:47:25,407 [INFO] Epoch and iter 26 1883 Loss 26.806610107421875 Time 102.02498936653137
2024-08-15 13:49:07,440 [INFO] Epoch and iter 26 1983 Loss 16.24718475341797 Time 102.03284668922424
2024-08-15 13:50:49,473 [INFO] Epoch and iter 26 2083 Loss 23.38949966430664 Time 102.03226733207703
2024-08-15 13:52:31,523 [INFO] Epoch and iter 26 2183 Loss 15.420881271362305 Time 102.04882454872131
2024-08-15 13:54:13,546 [INFO] Epoch and iter 26 2283 Loss 19.81631851196289 Time 102.02274584770203
2024-08-15 13:55:55,586 [INFO] Epoch and iter 26 2383 Loss 19.714614868164062 Time 102.03971362113953
2024-08-15 13:57:37,625 [INFO] Epoch and iter 26 2483 Loss 18.925230026245117 Time 102.0375337600708
2024-08-15 13:59:19,646 [INFO] Epoch and iter 26 2583 Loss 17.935659408569336 Time 102.02036595344543
2024-08-15 14:01:01,663 [INFO] Epoch and iter 26 2683 Loss 18.05171012878418 Time 102.01680827140808
2024-08-15 14:02:43,685 [INFO] Epoch and iter 26 2783 Loss 18.539649963378906 Time 102.02176022529602
2024-08-15 14:04:25,706 [INFO] Epoch and iter 26 2883 Loss 12.296201705932617 Time 102.02012395858765
2024-08-15 14:06:07,736 [INFO] Epoch and iter 26 2983 Loss 17.43224334716797 Time 102.02907514572144
2024-08-15 14:07:49,755 [INFO] Epoch and iter 26 3083 Loss 17.638486862182617 Time 102.01889324188232
2024-08-15 14:09:31,776 [INFO] Epoch and iter 26 3183 Loss 14.596835136413574 Time 102.01985764503479
2024-08-15 14:11:13,785 [INFO] Epoch and iter 26 3283 Loss 12.16038703918457 Time 102.00865530967712
2024-08-15 14:12:55,815 [INFO] Epoch and iter 26 3383 Loss 18.631099700927734 Time 102.02926301956177
2024-08-15 14:14:37,862 [INFO] Epoch and iter 26 3483 Loss 12.221736907958984 Time 102.04650354385376
2024-08-15 14:16:01,343 [INFO] Saving this epoch  26 +1
2024-08-15 14:16:01,416 [INFO] Epoch 27 started at Thu Aug 15 14:16:01 2024
2024-08-15 14:16:19,863 [INFO] Epoch and iter 27 17 Loss 15.54173755645752 Time 102.00037336349487
2024-08-15 14:18:01,903 [INFO] Epoch and iter 27 117 Loss 17.1966552734375 Time 102.03845000267029
2024-08-15 14:19:43,948 [INFO] Epoch and iter 27 217 Loss 10.235845565795898 Time 102.04431414604187
2024-08-15 14:21:25,985 [INFO] Epoch and iter 27 317 Loss 12.925148010253906 Time 102.03672909736633
2024-08-15 14:23:08,008 [INFO] Epoch and iter 27 417 Loss 17.41349983215332 Time 102.02184772491455
2024-08-15 14:24:50,042 [INFO] Epoch and iter 27 517 Loss 12.953516006469727 Time 102.03323578834534
2024-08-15 14:26:32,081 [INFO] Epoch and iter 27 617 Loss 23.361055374145508 Time 102.03877091407776
2024-08-15 14:28:14,117 [INFO] Epoch and iter 27 717 Loss 17.612173080444336 Time 102.03577876091003
2024-08-15 14:29:56,149 [INFO] Epoch and iter 27 817 Loss 15.844497680664062 Time 102.03082084655762
2024-08-15 14:31:38,187 [INFO] Epoch and iter 27 917 Loss 11.991781234741211 Time 102.03765845298767
2024-08-15 14:33:20,227 [INFO] Epoch and iter 27 1017 Loss 15.501956939697266 Time 102.03915309906006
2024-08-15 14:35:02,258 [INFO] Epoch and iter 27 1117 Loss 17.093624114990234 Time 102.02982020378113
2024-08-15 14:36:44,301 [INFO] Epoch and iter 27 1217 Loss 14.638629913330078 Time 102.04326128959656
2024-08-15 14:38:26,332 [INFO] Epoch and iter 27 1317 Loss 14.438590049743652 Time 102.02961778640747
2024-08-15 14:40:08,381 [INFO] Epoch and iter 27 1417 Loss 19.411983489990234 Time 102.04882764816284
2024-08-15 14:41:50,413 [INFO] Epoch and iter 27 1517 Loss 18.330533981323242 Time 102.03057336807251
2024-08-15 14:43:32,443 [INFO] Epoch and iter 27 1617 Loss 15.477493286132812 Time 102.03019618988037
2024-08-15 14:45:14,500 [INFO] Epoch and iter 27 1717 Loss 14.93686580657959 Time 102.05573296546936
2024-08-15 14:46:56,540 [INFO] Epoch and iter 27 1817 Loss 16.357391357421875 Time 102.03981375694275
2024-08-15 14:48:38,584 [INFO] Epoch and iter 27 1917 Loss 13.399147033691406 Time 102.04326725006104
2024-08-15 14:50:20,633 [INFO] Epoch and iter 27 2017 Loss 15.6175537109375 Time 102.04818296432495
2024-08-15 14:52:02,673 [INFO] Epoch and iter 27 2117 Loss 16.016300201416016 Time 102.03947496414185
2024-08-15 14:53:44,702 [INFO] Epoch and iter 27 2217 Loss 16.755531311035156 Time 102.02818393707275
2024-08-15 14:55:26,737 [INFO] Epoch and iter 27 2317 Loss 15.561120986938477 Time 102.03394150733948
2024-08-15 14:57:08,776 [INFO] Epoch and iter 27 2417 Loss 19.75551414489746 Time 102.0385525226593
2024-08-15 14:58:50,833 [INFO] Epoch and iter 27 2517 Loss 15.899147033691406 Time 102.05660915374756
2024-08-15 15:00:32,887 [INFO] Epoch and iter 27 2617 Loss 17.192344665527344 Time 102.05297064781189
2024-08-15 15:02:14,939 [INFO] Epoch and iter 27 2717 Loss 13.795286178588867 Time 102.05129194259644
2024-08-15 15:03:56,987 [INFO] Epoch and iter 27 2817 Loss 19.08612823486328 Time 102.04760813713074
2024-08-15 15:05:39,014 [INFO] Epoch and iter 27 2917 Loss 18.9766788482666 Time 102.02653980255127
2024-08-15 15:07:21,061 [INFO] Epoch and iter 27 3017 Loss 17.857946395874023 Time 102.04627799987793
2024-08-15 15:09:03,134 [INFO] Epoch and iter 27 3117 Loss 20.700437545776367 Time 102.07205605506897
2024-08-15 15:10:45,176 [INFO] Epoch and iter 27 3217 Loss 20.065933227539062 Time 102.04137253761292
2024-08-15 15:12:27,213 [INFO] Epoch and iter 27 3317 Loss 12.109127044677734 Time 102.03651762008667
2024-08-15 15:14:09,280 [INFO] Epoch and iter 27 3417 Loss 14.678300857543945 Time 102.06605553627014
2024-08-15 15:15:51,339 [INFO] Epoch and iter 27 3517 Loss 17.385555267333984 Time 102.05798697471619
2024-08-15 15:16:40,136 [INFO] Saving this epoch  27 +1
2024-08-15 15:16:40,208 [INFO] Epoch 28 started at Thu Aug 15 15:16:40 2024
2024-08-15 15:17:33,371 [INFO] Epoch and iter 28 51 Loss 20.167638778686523 Time 102.03214025497437
2024-08-15 15:19:15,462 [INFO] Epoch and iter 28 151 Loss 15.687501907348633 Time 102.0893452167511
2024-08-15 15:20:57,585 [INFO] Epoch and iter 28 251 Loss 22.032119750976562 Time 102.12247204780579
2024-08-15 15:22:39,702 [INFO] Epoch and iter 28 351 Loss 19.1522274017334 Time 102.11624789237976
2024-08-15 15:24:21,827 [INFO] Epoch and iter 28 451 Loss 15.827139854431152 Time 102.12458658218384
2024-08-15 15:26:03,913 [INFO] Epoch and iter 28 551 Loss 9.636087417602539 Time 102.08542585372925
2024-08-15 15:27:46,017 [INFO] Epoch and iter 28 651 Loss 15.49700927734375 Time 102.10272288322449
2024-08-15 15:29:28,094 [INFO] Epoch and iter 28 751 Loss 16.180896759033203 Time 102.07647156715393
2024-08-15 15:31:10,192 [INFO] Epoch and iter 28 851 Loss 16.541667938232422 Time 102.09788775444031
2024-08-15 15:32:52,296 [INFO] Epoch and iter 28 951 Loss 17.29633140563965 Time 102.10295557975769
2024-08-15 15:34:34,407 [INFO] Epoch and iter 28 1051 Loss 19.420867919921875 Time 102.11078715324402
2024-08-15 15:36:16,513 [INFO] Epoch and iter 28 1151 Loss 14.258992195129395 Time 102.10509371757507
2024-08-15 15:37:58,615 [INFO] Epoch and iter 28 1251 Loss 18.990001678466797 Time 102.1011254787445
